[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.\n\n\n\n\n\n\nKey to books used below\n\n\n\n\nISL = An Introduction to Statistical Learning with Python, by James, Witten, Hastie, and Tibshirani\nUDL = Understanding Deep learning by Simon Prince\nDM = Introduction to Data Mining, by Tan, Steinbach, Karpatne, and Kumar\nPRML = Pattern Recognition and Machine Learning, by Bishop\nDL = Deep Learning, by Goodfellow, Bengio, and Courville\nRL = Reinforcement Learning: An Introduction: An Introduction, by Sutton and Barto\n\n\n\n\n\n  \n    \n      Event Type\n      Date\n      Description\n      Readings\n      Course Materials\n    \n  \n\n  \n    Lecture 1\n    Thursday Jan 11\n    \n      What is machine learning? \n      Course overview and an orientation to the major branches of machine learning: supervised, unsupervised, and reinforcement learning \n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    Monday  Jan 15\n    Martin Luther King Jr. Day\n    \n    \n  \n\n  \n    \n    \n    Module 1: Supervised Learning\n    \n    \n  \n\n  \n    Lecture 2\n    Tuesday  Jan 16\n    \n      An end-to-end machine learning example \n      An introduction to formulating a supervised machine learning problem. Stating the problem, creating the model, evaluating performance, and operationalizing the solution. \n    ISL Ch. 1 + 2.1Watch this lecture \n    \n      [slides]\n      \n      \n    \n  \n\n  \n    Lecture 3\n    Thursday  Jan 18\n    \n      How flexible should my algorithms be: the bias-variance tradeoff  \n      K-nearest neighbors classification and the bias-variance tradeoff \n    \n    ISL 2.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Monday Jan 22\n    Assignment #1 Due (at 9pm) Probability, Linear Algebra, & Computational Programming\n    \n    \n      \n      \n      \n      [submit]\n    \n  \n\n  \n    Lecture 4\n    Tuesday  Jan 23\n    \n      Linear Models I \n      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification\n    \n    ISL Intro of 3, 3.1, and 3.2\n    \n      [slides]\n      \n    \n  \n\n\n\n  \n    Lecture 5\n    Thursday  Jan 25\n    \n      Linear Models II \n      Nonlinear transformations of predictors; linear models for classification including the perceptron and logistic regression; cost/loss functions for classification (cross entropy loss); introduction to gradient descent.\n    \n    ISL 3.3 and 3.5\n    \n      [slides]\n      \n    \n  \n  \n  \n    Lecture 6\n    Tuesday  Jan 30\n    \n      Performance evaluation and model comparison \n      Choosing the right model: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, and confusion matrices\n    \n    ISL 4.1, 4.2, and 4.3\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 7\n    Thursday  Feb 1\n    \n      Resampling methods for performance evaluation: model validation  and testing strategies \n      How to use model performance metrics to measure metrics of generalization performance; resampling techniques: training, testing, and validation datasets and cross validation; common pitfalls around biased sampling and data snooping/leakage\n    \n    ISL 5.1 and 5.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     MondayFeb 5\n    Assignment #2 Due (at 9pm)Supervised Machine Learning Fundamentals\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 8\n    Tuesday  Feb 6\n    \n      Decision theory \n      A risk-based framework for determining to operate supervised learning algorithms in practice; choosing ROC operating points through risk-minimization and how application-specific costs associated with different types of errors can be used to determine optimal operating points for classifiers\n    \n    \n      \n      Link to reading\n      \n    \n    \n      [slides]\n      \n    \n  \n\n    \n    Lecture 9\n    Thursday  Feb 8\n    \n      Reducing overfit \n      Feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization; early stopping.\n    \n    ISL 6.1 and 6.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 10\n    Tuesday  Feb 13\n    \n      Generative models for classification \n      Generative vs discriminative models; linear discriminant analysis, quadratic discriminant analysis, and naïve Bayes\n    \n    ISL 4.4 and 4.5\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 11\n    Thursday  Feb 15\n    \n      Tree-based models and ensembles \n      From decision trees to random forests: bagging, bootstrapping, and boosting\n    \n    ISL 8.1 and 8.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Monday  Feb 19\n    Assignment #3 Due (at 9pm)Supervised learning model training and evaluation\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 12\n    Tuesday  Feb 20\n    \n      Kernel Methods \n      Introducing Kernel machines via the kernel perceptron, maximum margin classifiers, and support vector machines\n    \n    ISL Ch 9: 9.1-9.4\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 13\n    Thursday  Feb 22\n    \n      Neural networks I \n      Introduction to neural networks and representation learning; forward propagation, network architecture, and how to adapt to regression or classification problems\n    \n    UDL Ch 3: 3.1, 3.2; PRML Ch 5: 5.1 \n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 14\n    Tuesday  Feb 27\n    \n      Neural networks II \n      Fitting a neural network to training data through gradient descent and backpropagation; how backpropagation is used to compute gradients in neural networks; hyperparameters and architecture choices in neural networks and practices for training neural networks warningfully\n    \n    UDL Ch 3: 6.1; PRML Ch 5: 5.3.1, and Calculus on Computational Graphs\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 15\n    Thursday  Feb 29\n    \n      Introduction to Deep learning \n      Common architectures of deep learning models, in particular convolutional neural networks for computer vision and the tools used to implement them\n    \n    DL Ch 11: Practical Methodology\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 2: Unsupervised Learning\n    \n    \n  \n\n  \n    Lecture 16\n    TuesdayMar 5\n    \n      Dimensionality reduction \n      The Curse of Dimensionality and intro to principal components analysis (PCA)\n    \n    ISL 6.3, 6.4, 12.1, and 12.2 \n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Monday  Mar 4\n    Assignment #4 Due (at 9pm)Neural Networks\n    \n    \n      [assignment] \n      [submit]\n      \n      \n      \n    \n  \n\n  \n    Lecture 17\n    ThursdayMar 7\n    \n      Principal components analysis (PCA) \n      Explaining how PCA works and how we calculate the principal components.\n    \n    ISL 12.4\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Friday  Mar 8 \n    Project Proposal Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    \n    Mar 9-16\n    Spring Break Week\n    \n    \n  \n\n  \n    Lecture 18\n    Tuesday  Mar 19\n    \n      Clustering I \n      From K-means to Gaussian mixture model clustering and Expectation Maximization\n    \n    DM Ch 7 (link): Intro, 7.1 and 7.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 19\n    Thursday  Mar 21\n    \n      Clustering II \n      Hierarchical clustering, DBSCAN, and spectral clustering\n    \n   DM Ch 7 (link): 7.3 and 7.4\n   \n     [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 3: Reinforcement Learning\n    \n    \n  \n\n  \n    Lecture 20\n    Tuesday  Mar 26\n    \n      Reinforcement Learning I \n      Formulating the reinforcement learning problem\n    \n    RL Ch 1: 1.1-1.6; Ch 2: 2.1-2.5\n    \n      [slides] \n      \n    \n  \n\n  \n    Lecture 21\n    Thursday  Mar 28\n    \n      Reinforcement Learning II \n      Policy and value functions, rewards, and introduction to Markov processes \n    \n    RL Ch 3\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Monday  Mar 25\n    Assignment #5 Due (at 9pm)  Kaggle Competition and Unsupervised LearningKaggle Competition Ends 9pm on Sun Mar 24\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 22\n    Tuesday Apr 2\n    \n      Reinforcement Learning III \n      From Markov Chains to Markov Decision Processes (MDPs)\n    \n    RL Ch 4\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 3 \n    Draft Final Project Report Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    Lecture 23\n    Thursday  Apr 4\n    \n      Reinforcement Learning IV \n      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods\n    \n    RL Ch 5: 5.1-5.3\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 4: Machine Learning Trends, Practical Considerations, and Advanced Topics\n    \n    \n  \n\n  \n    Lecture 24\n    Tuesday  Apr 11\n    \n      Advanced topics and applications I \n      A survey of advanced topics including semi- and self-supervised learning\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 25\n    Thursday  Apr 9\n    \n      Advanced topics and applications II \n      Discussion on where the field is heading and how to stay up-to-date\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Tuesday  Apr 16\n    \n      Final project showcase(last class meeting of the semester)\n    \n    \n    \n      [project]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 17 \n    Final Project Report Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Thursday  Apr 18\n    Final Project Peer Evaluation Due (at 9pm)\n    \n    \n      [project]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 24\n    (Optional) Assignment #6 Due (at 9pm)Reinforcement learning\n    \n    \n      [assignment]"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Logistics",
    "section": "",
    "text": "Class Time and Location\n\n\n\n\nWhen: Tuesday and Thursdays 10:15am - 11:30am\nWhere: French Science 2231\n\n\n\n\n\n\nEd Discussions: Announcements, Q&A on course content (assignments, quizzes, grades), ALL course communications\nGradescope: Quizzes, assignments, and project submission & feedback\nSchedule: Schedule of class topics and deliverables\nCanvas: Posted grades\n\n\n\n\nA version of each book is available free online:\n\nAn Introduction to Statistical Learning with Python by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2013.\nUnderstanding Deep learning by Simon Prince, 2023.\nPattern Recognition and Machine Learning by Christopher Bishop, 2006.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016.\nReinforcement Learning: An Introduction, by Richard Sutton and Andrew Barto, 2018.\n\n\n\n\nAssignments, projects, & quizzes: Assignments and projects details are posted on the course syllabus. For expectations and instructions on the assignments, see the Assignment Instructions. Quizzes are found on Gradescope and are due prior to the start of the lecture for which they’re titled (i.e. the Lecture 3 Quiz is due by the start of Lecture 3).\n\n\n\n55% Assignments (5, each worth ~11%)\n20% Quizzes (~23, each worth &lt;1%)\n25% Final Project\n\n\n\n\n\nThis course moves quickly, so having a firm grasp on prerequisites is important. The prerequisites are as follows:\n\nProgramming: Fundamentals of Python programming.\nMathematics: Calculus and linear algebra.\nStatistics: Introductory probability and statistics."
  },
  {
    "objectID": "syllabus.html#basic-logistics",
    "href": "syllabus.html#basic-logistics",
    "title": "Course Logistics",
    "section": "",
    "text": "Class Time and Location\n\n\n\n\nWhen: Tuesday and Thursdays 10:15am - 11:30am\nWhere: French Science 2231\n\n\n\n\n\n\nEd Discussions: Announcements, Q&A on course content (assignments, quizzes, grades), ALL course communications\nGradescope: Quizzes, assignments, and project submission & feedback\nSchedule: Schedule of class topics and deliverables\nCanvas: Posted grades\n\n\n\n\nA version of each book is available free online:\n\nAn Introduction to Statistical Learning with Python by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2013.\nUnderstanding Deep learning by Simon Prince, 2023.\nPattern Recognition and Machine Learning by Christopher Bishop, 2006.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016.\nReinforcement Learning: An Introduction, by Richard Sutton and Andrew Barto, 2018.\n\n\n\n\nAssignments, projects, & quizzes: Assignments and projects details are posted on the course syllabus. For expectations and instructions on the assignments, see the Assignment Instructions. Quizzes are found on Gradescope and are due prior to the start of the lecture for which they’re titled (i.e. the Lecture 3 Quiz is due by the start of Lecture 3).\n\n\n\n55% Assignments (5, each worth ~11%)\n20% Quizzes (~23, each worth &lt;1%)\n25% Final Project\n\n\n\n\n\nThis course moves quickly, so having a firm grasp on prerequisites is important. The prerequisites are as follows:\n\nProgramming: Fundamentals of Python programming.\nMathematics: Calculus and linear algebra.\nStatistics: Introductory probability and statistics."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Course Logistics",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic dishonesty\nAdherence to the Duke Community Standard is expected. To uphold the Duke Community Standard:\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised\nAnyone found in violation of the Standard will be reported to the Office of Student Conduct.\n\n\nClass Attendance\nAttending class is a vital component of the course as it is one of the multiple ways in which you will interact with and learn course material. In person class attendance is therefore expected for this course. For any special circumstances, please reach out to the course instructor.\n\n\nSick absences\nTo keep the university community as safe and healthy as possible, please do not come to class if you have cold symptoms. Please inform me of your absence and plan to complete any missed work. Students who encounter short- and long-term medical issues or instances of personal distress or emergency can seek academic support if needed. Recordings of the class will be available for excused absences.\n\n\nAccommodations and accessibility\nIf you need special accommodations due to physical or learning disabilities, medical needs, religious practices, or other reasons, please inform us as soon as possible so we can work to accommodate those needs.\nIf you are a student with a disability and need accommodations for this class, please register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to your instructor. Please contact SDAO for more information: sdao@duke.edu or .\n\n\nLate Submissions\nAssignments and projects are due in class by the start of class on the date posted. Late deliverables will ONLY be accepted at the discretion of the instructor. Late Assignments will result in a reduction of 5 points off the grade per day late. Course projects will not be accepted after the deadline. Quizzes will not be accepted after the deadline. While quizzes cannot be made up since the answers are discussed, the lowest two quizzes will be dropped at the end of the semester for each student to accommodate necessary absences and to account for days when we’re off our game. Please reach out to the TA’s or instructor as early as possible to request any special accommodations.\n\n\nCollaboration\nThere will be three modes of collaboration ranging from fully-collaborative group projects, to fully-independent work. The three modes are as follows, and will be indicated throughout the course:\n\nMode 1: Team-based Assignment. Collaboration is expected with every member of the team contributing to a single deliverable. Applies to the Project\nMode 2: Individual Assignment – Collaboration Permitted. Students hand in individual work, but they may work with others if they provide citations of the help they received, such as a list of people who assisted/collaborated with them to produce the final product. Duplication or copying is not permissible, even in part, and constitutes a breach of the honor code. Applies to Assignments\nMode 3: Individual Assignment – No Collaboration Permitted. Students hand in individual work that is completed entirely independent of any discussion or help from other students. Clarifying questions to teaching assistants and instructors are both permissible and encouraged. Applies to Quizzes\n\n\n\nRules for recording course content\nStudent recording recordings of lectures must be permitted by the instructor prior to recording and shall be for private study only. Such recordings shall not be distributed to anyone else without authorization by the instructor whose lecture has been recorded. However, the instructor may arrange through the Office of Information Technology to make recorded lectures available to students enrolled in the class on such terms and conditions as he or she prescribes. Redistribution of recorded lectures is prohibited. Unauthorized distribution is a cause for disciplinary action by the Judicial Board. The full policy on recoding of lectures falls under the Duke University Policy on Intellectual Property Rights, available here."
  },
  {
    "objectID": "syllabus.html#the-use-of-artificial-intelligence",
    "href": "syllabus.html#the-use-of-artificial-intelligence",
    "title": "Course Logistics",
    "section": "The use of Artificial Intelligence",
    "text": "The use of Artificial Intelligence\nThis class is all about artificial intelligence, however, the primary intelligence that we are working to expand is your own.\n\nChallenges of commercial AI tools\nQuoted excerpts below from the University of Delaware\nCommercial AI tools have significant and sometimes difficult-to-spot limitations. “For example, the current generation of Chat-GPT cannot perform synthesis of sources or even document the sources that were used to create its responses. So it may be useful for idea generation or generic recommendations but it has no ability to creatively and independently generate new insights and make novel connections. These tools may also produce information that is incorrect or biased so students must ensure that material they submit or share has been verified with appropriate sources and is reasonably free of biases.”\n\n\nResponsible use of AI tools\nThere are a number of ways that AI tools can be helpful:\n\nAs a way of detecting spelling and grammar issues (such as with Grammarly)\nAs a way of identifying possible ways to structure writing (although NOT to do the writing for you)\nAs a way of brainstorming ideas (keeping in mind that what is generated by the AI may be incorrect or misleading)\n\nIn this class, the use of AI tools is acceptable WITH CITATION. An appropriate citation would indicate the following information:\n\nWhat AI tool you used (e.g. ChatGPT)\nHow you used it, specifying prompts used (e.g. I checked my writeup for clarity with the prompt, “Review this text {} and improve its clarity”)\n\nAt the end of the day you are responsible for the correctness of everything that you submit, regardless of whether there was an AI tool used in its generation."
  },
  {
    "objectID": "syllabus.html#mental-health-and-wellness-resources",
    "href": "syllabus.html#mental-health-and-wellness-resources",
    "title": "Course Logistics",
    "section": "Mental Health and Wellness Resources",
    "text": "Mental Health and Wellness Resources\nStudent mental health and wellness are of primary importance at Duke, and the university offers resources to support students in managing daily stress and self- care.\nIf your mental health concerns or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke offers several resources for all students to seek assistance and to nurture daily habits that support overall well-being, some of which are listed below:\n\nDuWell, (919) 681-8421. DuWell provides Moments of Mindfulness (stress management and resilience building) and meditation programming (Koru workshop) to assist students in developing a daily emotional well-being practice. All are welcome and no experience necessary.\nDukeReach. DukeReach provides comprehensive outreach services to identify and support students in managing all aspects of well-being.\nCounseling and Psychological Services (CAPS), (919) 660-1000. CAPS services include individual and group counseling services, psychiatric services, and workshops. CAPS also provides referral to off-campus resources for specialized care. • TimelyCare (formerly known as Blue Devils Care). An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling."
  },
  {
    "objectID": "index.html#course-summary",
    "href": "index.html#course-summary",
    "title": "Overview",
    "section": "Course Summary",
    "text": "Course Summary\nIn almost every field, there is a need to draw inference from or make decisions based on data. The goal of this course is to provide an introduction to machine learning that is approachable to diverse disciplines and empowers students to become proficient in the foundational concepts and tools. You will learn to (a) structure a machine learning problems and determine which algorithmic tools are appropriate, (b) evaluate the performance of your solution using field-appropriate metrics and practices, and (c) accurately interpret your model output and communicate your results to interdisciplinary audiences. This course is a fast-paced, applied introduction to machine learning that through extensive practice with foundational tools, helps you to develop your knowledge of foundational machine learning concepts, and provides practical experience with those tools to prepare you for practice or future study."
  },
  {
    "objectID": "index.html#detailed-description",
    "href": "index.html#detailed-description",
    "title": "Overview",
    "section": "Detailed description",
    "text": "Detailed description\nMachine learning is a collection of useful tools for understanding and making decisions based on data and past experience; it is not a hammer to be applied to every nail, but rather a precision tool to be used when needed. This course will begin with exploring the purpose of machine learning told through a discussion of the types of problems that machine learning can answer: describing, predicting, and strategizing based on data and the tools at our disposal to address these challenges: supervised learning including classification and regression; unsupervised learning including clustering and density estimation; and reinforcement learning. There will be a strong focus on how to formulate a machine learning problem. Central to that formulation will be developing an understanding of how to preprocess data for analysis (e.g. feature extraction/dimensionality reduction, training/validation data sampling), model selection, and performance evaluation with cross validation. The final topic of this course will be a brief overview of state-of-the-art machine learning techniques that are emerging in the field.\nThroughout this course, the focus will be on applying algorithms rather than diving deeply into theory. You will be asked to consider the practical issues of machine learning problem solving: challenges of applying machine learning code packages, striving for parsimony (simplicity of models) and interpretability, and ensuring model assumptions are valid for a given problem and dataset. This course will also stress the importance of team-based collaboration, the value of producing fully reproducible and validated results, and tools to help with both such as version control and code repositories.\nCommunicating your results. Data science solutions are only as impactful as the communicator who shares them: therefore communication of your findings will be a core component of this course. Demonstrating competency in data science means (a) exhibiting a working knowledge of technical concepts including programming, statistics, and mathematics and (b) being able to clearly communicate the problem you were trying to solve or question you were trying to answer, why it matters, and how well your analysis worked. You will have opportunities to practice these skills throughout this course in the context of interpreting and sharing the results of your analyses."
  },
  {
    "objectID": "assignments/Assignment_1.html",
    "href": "assignments/Assignment_1.html",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "Netid: Your netid here\nNames of students you worked with on this assignment: LIST HERE IF APPLICABLE (delete if not)\nNote: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information.\nInstructions for all assignments can be found here, and are also linked to from the course syllabus.\nTotal points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality."
  },
  {
    "objectID": "assignments/Assignment_1.html#your-full-name-here",
    "href": "assignments/Assignment_1.html#your-full-name-here",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "Netid: Your netid here\nNames of students you worked with on this assignment: LIST HERE IF APPLICABLE (delete if not)\nNote: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information.\nInstructions for all assignments can be found here, and are also linked to from the course syllabus.\nTotal points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality."
  },
  {
    "objectID": "assignments/Assignment_1.html#section",
    "href": "assignments/Assignment_1.html#section",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "1",
    "text": "1\n[3 points]\nLet \\(f(x) = \\begin{cases}  0 & x &lt; 0 \\\\  \\alpha x^2 & 0 \\leq x \\leq 2 \\\\  0 & 2 &lt; x  \\end{cases}\\)\nFor what value of \\(\\alpha\\) is \\(f(x)\\) a valid probability density function?\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-1",
    "href": "assignments/Assignment_1.html#section-1",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "2",
    "text": "2\n[3 points] What is the cumulative distribution function (CDF) that corresponds to the following probability distribution function? Please state the value of the CDF for all possible values of \\(x\\).\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-2",
    "href": "assignments/Assignment_1.html#section-2",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "3",
    "text": "3\n[6 points] For the probability distribution function for the random variable \\(X\\),\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nwhat is the (a) expected value and (b) variance of \\(X\\). Show all work.\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-3",
    "href": "assignments/Assignment_1.html#section-3",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "4",
    "text": "4\n[6 points] You are given the training data below and asked to determine the probability that a sample of \\(x=0.54\\) comes from class 1, or equivalently, \\(P(Y=1|X=0.54)\\). The feature,, \\(x\\) can take on values between 0 and 1.\n\n\n\n\n\n\n\n\n\\(x\\) value range\nNegative data in the range (\\(x\\),\\(y=0\\))\nPositive data in the range (\\(x\\),\\(y=1\\))\n\n\n\n\n0.0 - 0.1\n(0.05,0),(0.07,0)\nNone\n\n\n0.1 - 0.2\n(0.11,0),(0.13,0),(0.19,0)\n(0.14,1)\n\n\n0.2 - 0.3\n(0.23,0)\n(0.24,1)\n\n\n0.3 - 0.4\n(0.35,0), (0.37,0)\n(0.32,1)\n\n\n0.4 - 0.5\n(0.49,0)\n(0.47,1)\n\n\n0.5 - 0.6\n(0.51,0)\n(0.53,1)\n\n\n0.6 - 0.7\nNone\n(0.61,1)\n\n\n0.7 - 0.8\nNone\n(0.77,1)\n\n\n0.8 - 0.9\nNone\n(0.83,1)\n\n\n0.9 - 1.0\nNone\n(0.92,1),(0.98,1)\n\n\n\nNote: You don’t need to use these data directly, but this provides an example of how the data could be distributed to form the empirical likelihoods and priors shown below.\nRecall Bayes’ Rule which computes the poster distribution, \\(P(Y|X)\\), based on the likelihood of the data conditioned on the class of the samples, \\(P(X|Y=y)\\), the prior \\(P(Y)\\) which is essentially the distribution of the class labels across all the data, and the evidence \\(P(X)\\) which is the distribution of the features, \\(X\\), regardless of class labels.\n\\[\\begin{equation}\nP(Y|X)= \\frac{P(X|Y)P(Y)}{P(X)}\n\\end{equation}\\]\nAlso, note that P(X) can alternatively be computed in this case as:\n\\[\\begin{equation}\nP(X) = P(X|Y=0)P(Y=0) + P(X|Y=1)P(Y=1)\n\\end{equation}\\]\nBelow are the prior and the likelihood functions based on the dataset above:\n\n\n\nWhat is \\(P(X=0.54|Y=1)P(Y=1)\\)?\nWhat is \\(P(X=0.54)\\)?\nWhat is \\(P(Y=1|X=0.54)\\)?\n\nShow each value you use for each computation.\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-5",
    "href": "assignments/Assignment_1.html#section-5",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "6",
    "text": "6\n[14 points] Matrix manipulations and multiplication. Machine learning involves working with many matrices, so this exercise will provide you with the opportunity to practice those skills.\nLet \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} -1 \\\\ 3 \\\\ 8 \\end{bmatrix}\\), \\(\\mathbf{c} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 6 \\end{bmatrix}\\), and \\(\\mathbf{I} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\nCompute the following using Python or indicate that it cannot be computed. Refer to NumPy’s tools for handling matrices. While all answers should be computer using Python, your response to whether each item can be computed should refer to underlying linear algebra. There may be circumstances when Python will produce an output, but based on the dimensions of the matrices involved, the linear algebra operation is not possible. For the case when an operation is invalid, explain why it is not.\nWhen the quantity can be computed, please provide both the Python code AND the output of that code (this need not be in LaTex)\n\n\\(\\mathbf{A}\\mathbf{A}\\)\n\\(\\mathbf{A}\\mathbf{A}^T\\)\n\\(\\mathbf{A}\\mathbf{b}\\)\n\\(\\mathbf{A}\\mathbf{b}^T\\)\n\\(\\mathbf{b}\\mathbf{A}\\)\n\\(\\mathbf{b}^T\\mathbf{A}\\)\n\\(\\mathbf{b}\\mathbf{b}\\)\n\\(\\mathbf{b}^T\\mathbf{b}\\)\n\\(\\mathbf{b}\\mathbf{b}^T\\)\n\\(\\mathbf{b} + \\mathbf{c}^T\\)\n\\(\\mathbf{b}^T\\mathbf{b}^T\\)\n\\(\\mathbf{A}^{-1}\\mathbf{b}\\)\n\\(\\mathbf{A}\\circ\\mathbf{A}\\)\n\\(\\mathbf{b}\\circ\\mathbf{c}\\)\n\nNote: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol “\\(\\circ\\)”.\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-6",
    "href": "assignments/Assignment_1.html#section-6",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "7",
    "text": "7\n[8 points] Eigenvectors and eigenvalues. Eigenvectors and eigenvalues are useful for some machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning and in this course we will encounter them in relation to Principal Components Analysis (PCA), clustering algorithms, For an intuitive review of these concepts, explore this interactive website at Setosa.io. Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube here. For these questions, numpy may once again be helpful.\n\nCalculate the eigenvalues and corresponding eigenvectors of matrix \\(\\mathbf{A}\\) above, from the last question.\nChoose one of the eigenvector/eigenvalue pairs, \\(\\mathbf{v}\\) and \\(\\lambda\\), and show that \\(\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\\). This relationship extends to higher orders: \\(\\mathbf{A} \\mathbf{A} \\mathbf{v} = \\lambda^2 \\mathbf{v}\\)\nShow that the eigenvectors are orthogonal to one another (e.g. their inner product is zero). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. We will see this property is used in PCA to identify the dimensions of greatest variation in our data when we discuss dimensionality reduction.\n\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-7",
    "href": "assignments/Assignment_1.html#section-7",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "8",
    "text": "8\n[10 points] Loading data and gathering insights from a real dataset\nIn data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\nData. The data for this problem can be found in the data subfolder in the assignments folder on github. The filename is a1_egrid2016.xlsx. This dataset is the Environmental Protection Agency’s (EPA) Emissions & Generation Resource Integrated Database (eGRID) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We’ll be using a subset of those data.\nThe fields we’ll be using include:\n\n\n\nfield\ndescription\n\n\n\n\nSEQPLT16\neGRID2016 Plant file sequence number (the index)\n\n\nPSTATABB\nPlant state abbreviation\n\n\nPNAME\nPlant name\n\n\nLAT\nPlant latitude\n\n\nLON\nPlant longitude\n\n\nPLPRMFL\nPlant primary fuel\n\n\nCAPFAC\nPlant capacity factor\n\n\nNAMEPCAP\nPlant nameplate capacity (Megawatts MW)\n\n\nPLNGENAN\nPlant annual net generation (Megawatt-hours MWh)\n\n\nPLCO2EQA\nPlant annual CO2 equivalent emissions (tons)\n\n\n\nFor more details on the data, you can refer to the eGrid technical documents. For example, you may want to review page 45 and the section “Plant Primary Fuel (PLPRMFL)”, which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\nThere also are a couple of “gotchas” to watch out for with this dataset: - The headers are on the second row and you’ll want to ignore the first row (they’re more detailed descriptions of the headers). - NaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with these sorts of missing values will be important.\nYour objective. For this dataset, your goal is to answer the following questions about electricity generation in the United States:\n(a) Which plant has generated the most energy (measured in MWh)?\n(b) What is the name of the northern-most power plant in the United States?\n(c) What is the state where the northern-most power plant in the United States is located?\n(d) Plot a bar plot showing the amount of energy produced by each fuel type across all plants.\n(e) From the plot in (d), which fuel for generation produces the most energy (MWh) in the United States?\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-8",
    "href": "assignments/Assignment_1.html#section-8",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "9",
    "text": "9\n[6 points] Vectorization. When we first learn to code and think about iterating over an array, we often use loops. If implemented correctly, that does the trick. In machine learning, we iterate over so much data that those loops can lead to significant slow downs if they are not computationally efficient. In Python, vectorizing code and relying on matrix operations with efficient tools like numpy is typically the faster approach. Of course, numpy relies on loops to complete the computation, but this is at a lower level of programming (typically in C), and therefore is much more efficient. This exercise will explore the benefits of vectorization. Since many machine learning techniques rely on matrix operations, it’s helpful to begin thinking about implementing algorithms using vector forms.\nBegin by creating an array of 10 million random numbers using the numpy random.randn module. Compute the sum of the squares of those random numbers first in a for loop, then using Numpy’s dot module to perform an inner (dot) product. Verify that your code produces the same output in each case. Time how long it takes to compute each and report the results and report the output. How many times faster is the vectorized code than the for loop approach? (Note - your results may vary from run to run).\nYour output should use the print() function as follows (where the # symbols represent your answers, to a reasonable precision of 4-5 significant figures):\nTime [sec] (non-vectorized): ######\nTime [sec] (vectorized):     ######\nThe vectorized code is ##### times faster than the nonvectorized code\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-9",
    "href": "assignments/Assignment_1.html#section-9",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "10",
    "text": "10\n[10 points] This exercise will walk through some basic numerical programming and probabilistic thinking exercises, two skills which are frequently used in machine learning for answering questions from our data. 1. Synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=2\\) and a standard deviation of \\(\\sigma=1\\). Call these observations from a random variable \\(X\\), and call the vector of observations that you generate, \\(\\textbf{x}\\). 2. Calculate the mean and standard deviation of \\(\\textbf{x}\\) to validate (1) and provide the result to a precision of four significant figures. 3. Plot a histogram of the data in \\(\\textbf{x}\\) with 30 bins 4. What is the 90th percentile of \\(\\textbf{x}\\)? The 90th percentile is the value below which 90% of observations can be found. 5. What is the 99th percentile of \\(\\textbf{x}\\)? 6. Now synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=0\\) and a standard deviation of \\(\\sigma=3\\). Call these observations from a random variable \\(Y\\), and call the vector of observations that you generate, \\(\\textbf{y}\\). 7. Create a new figure and plot the histogram of the data in \\(\\textbf{y}\\) on the same axes with the histogram of \\(\\textbf{x}\\), so that both histograms can be seen and compared. 8. Using the observations from \\(\\textbf{x}\\) and \\(\\textbf{y}\\), estimate \\(E[XY]\\)\nANSWER"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-10",
    "href": "assignments/Assignment_1.html#section-10",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "11",
    "text": "11\n[4 points] Git is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the course website. As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\nComplete the Atlassian Git tutorial, specifically the following listed sections. Try each concept that’s presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as Github or Duke’s Gitlab. 1. What is version control 2. What is Git 3. Install Git 4. Setting up a repository 5. Saving changes 6. Inspecting a repository 7. Undoing changes 8. Rewriting history 9. Syncing 10. Making a pull request 11. Using branches 12. Comparing workflows\nI also have created two videos on the topic to help you understand some of these concepts: Git basics and a step-by-step tutorial.\nAs an additional resource, Microsoft now offers a git tutorial on this topic as well.\nFor your answer, affirm that you either completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets.\nANSWER\nI, [your name here], affirm that I have [completed the above tutorial / I have previous experience that covers all the content in this tutorial]"
  },
  {
    "objectID": "assignments/Assignment_1.html#section-11",
    "href": "assignments/Assignment_1.html#section-11",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "12",
    "text": "12\n[15 points] Here you’ll bring together some of the individual skills that you demonstrated above and create a Jupyter notebook based blog post on your exploratory data analysis. Your goal is to identify a question or problem and to work towards solving it or providing additional information or evidence (data) related to it through your data analysis. Below, we walk through a process to follow for your analysis. Additionally, you can find an example of a well-done exploratory data analysis here from past years.\n\nFind a dataset that interests you and relates to a question or problem that you find intriguing.\nDescribe the dataset, the source of the data, and the reason the dataset was of interest. Include a description of the features, data size, data creator and year of creation (if available), etc. What question are you hoping to answer through exploring the dataset?\nCheck the data and see if they need to be cleaned: are there missing values? Are there clearly erroneous values? Do two tables need to be merged together? Clean the data so it can be visualized. If the data are clean, state how you know they are clean (what did you check?).\nPlot the data, demonstrating interesting features that you discover. Are there any relationships between variables that were surprising or patterns that emerged? Please exercise creativity and curiosity in your plots. You should have at least 4 plots exploring the data in different ways.\nWhat insights are you able to take away from exploring the data? Is there a reason why analyzing the dataset you chose is particularly interesting or important? Summarize this for a general audience (imagine your publishing a blog post online) - boil down your findings in a way that is accessible, but still accurate.\n\nHere your analysis will evaluated based on: 1. Motivation: was the purpose of the choice of data clearly articulated? Why was the dataset chosen and what was the goal of the analysis? 2. Data cleaning: were any issues with the data investigated and, if found, were they resolved? 3. Quality of data exploration: were at least 4 unique plots (minimum) included and did those plots demonstrate interesting aspects of the data? Was there a clear purpose and takeaway from EACH plot? 4. Interpretation: Were the insights revealed through the analysis and their potential implications clearly explained? Was there an overall conclusion to the analysis?\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html",
    "href": "assignments/assignment1.html",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "Netid: Your netid here\nNames of students you worked with on this assignment: LIST HERE IF APPLICABLE (delete if not)\nNote: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information.\nInstructions for all assignments can be found here, and are also linked to from the course syllabus.\nTotal points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality.\n\n\n\nThe purpose of this assignment is to provide a refresher on fundamental concepts that we will use throughout this course and provide an opportunity to develop skills in any of the related skills that may be unfamiliar to you. Through the course of completing this assignment, you will…\n\nRefresh you knowledge of probability theory including properties of random variables, probability density functions, cumulative distribution functions, and key statistics such as mean and variance.\nRevisit common linear algebra and matrix operations and concepts such as matrix multiplication, inner and outer products, inverses, the Hadamard (element-wise) product, eigenvalues and eigenvectors, orthogonality, and symmetry.\nPractice numerical programming, core to machine learning, by loading and filtering data, plotting data, vectorizing operations, profiling code speed, and debugging and optimizing performance. You will also practice computing probabilities based on simulation.\nDevelop or refresh your knowledge of Git version control, which will be a core tool used in the final project of this course\nApply your skills altogether through an exploratory data analysis to practice data cleaning, data manipulation, interpretation, and communication\n\nWe will build on these concepts throughout the course, so use this assignment as a catalyst to deepen your knowledge and seek help with anything unfamiliar.\nIf some references would be helpful on these topics, I would recommend the following resources: - Mathematics for Machine Learning by Deisenroth, Faisal, and Ong - Deep Learning; Part I: Applied Math and Machine Learning Basics by Goodfellow, Bengio, and Courville - The Matrix Calculus You Need For Deep Learning by Parr and Howard - Dive Into Deep Learning; Appendix: Mathematics for Deep Learning by Weness, Hu, et al.\nNote: don’t worry if you don’t understand everything in the references above - some of these books dive into significant minutia of each of these topics.\n\n\n\n\nNote: for all assignments, write out equations and math using markdown and LaTeX. I recommend that you complete the work on paper before typing up the final version. For this assignment show your math for questions 1-4, meaning that you should include any intermediate steps necessary to understand the logic of your solution. Most can be completed in 3-4 steps. Being proficient in expressing yourself clearly, sometimes mathematically, is a valuable skill to have as a data scientist\n\n\n[3 points]\nLet \\(f(x) = \\begin{cases}  0 & x &lt; 0 \\\\  \\alpha x^2 & 0 \\leq x \\leq 2 \\\\  0 & 2 &lt; x  \\end{cases}\\)\nFor what value of \\(\\alpha\\) is \\(f(x)\\) a valid probability density function?\nANSWER\n\n\n\n\n[3 points] What is the cumulative distribution function (CDF) that corresponds to the following probability distribution function? Please state the value of the CDF for all possible values of \\(x\\).\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nANSWER\n\n\n\n\n[6 points] For the probability distribution function for the random variable \\(X\\),\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nwhat is the (a) expected value and (b) variance of \\(X\\). Show all work.\nANSWER\n\n\n\n[6 points] You are given the training data below and asked to determine the probability that a sample of \\(x=0.54\\) comes from class 1, or equivalently, \\(P(Y=1|X=0.54)\\). The feature,, \\(x\\) can take on values between 0 and 1.\n\n\n\n\n\n\n\n\n\\(x\\) value range\nNegative data in the range (\\(x\\),\\(y=0\\))\nPositive data in the range (\\(x\\),\\(y=1\\))\n\n\n\n\n0.0 - 0.1\n(0.05,0),(0.07,0)\nNone\n\n\n0.1 - 0.2\n(0.11,0),(0.13,0),(0.19,0)\n(0.14,1)\n\n\n0.2 - 0.3\n(0.23,0)\n(0.24,1)\n\n\n0.3 - 0.4\n(0.35,0), (0.37,0)\n(0.32,1)\n\n\n0.4 - 0.5\n(0.49,0)\n(0.47,1)\n\n\n0.5 - 0.6\n(0.51,0)\n(0.53,1)\n\n\n0.6 - 0.7\nNone\n(0.61,1)\n\n\n0.7 - 0.8\nNone\n(0.77,1)\n\n\n0.8 - 0.9\nNone\n(0.83,1)\n\n\n0.9 - 1.0\nNone\n(0.92,1),(0.98,1)\n\n\n\nNote: You don’t need to use these data directly, but this provides an example of how the data could be distributed to form the empirical likelihoods and priors shown below.\nRecall Bayes’ Rule which computes the poster distribution, \\(P(Y|X)\\), based on the likelihood of the data conditioned on the class of the samples, \\(P(X|Y=y)\\), the prior \\(P(Y)\\) which is essentially the distribution of the class labels across all the data, and the evidence \\(P(X)\\) which is the distribution of the features, \\(X\\), regardless of class labels.\n\\[\\begin{equation}\nP(Y|X)= \\frac{P(X|Y)P(Y)}{P(X)}\n\\end{equation}\\]\nAlso, note that P(X) can alternatively be computed in this case as:\n\\[\\begin{equation}\nP(X) = P(X|Y=0)P(Y=0) + P(X|Y=1)P(Y=1)\n\\end{equation}\\]\nBelow are the prior and the likelihood functions based on the dataset above:\n\n\n\nWhat is \\(P(X=0.54|Y=1)P(Y=1)\\)?\nWhat is \\(P(X=0.54)\\)?\nWhat is \\(P(Y=1|X=0.54)\\)?\n\nShow each value you use for each computation.\nANSWER\n\n\n\n\n\n\n\n\n[5 points] A common task in machine learning is a change of basis: transforming the representation of our data from one space to another. A prime example of this is through the process of dimensionality reduction as in Principle Components Analysis where we often seek to transform our data from one space (of dimension \\(n\\)) to a new space (of dimension \\(m\\)) where \\(m&lt;n\\). Assume we have a sample of data of dimension \\(n=4\\) (as shown below) and we want to transform it into a dimension of \\(m=2\\).\n\\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}\\)\n\nWhat are the dimensions of a matrix, \\(\\mathbf{A}\\), that would linearly transform our sample of data, \\(\\mathbf{x}\\), into a space of \\(m=2\\) through the operation \\(\\mathbf{Ax}\\)?\nExpress this transformation in terms of the components of \\(\\mathbf{x}\\): \\(x_1\\), \\(x_2\\), \\(x_3\\), \\(x_4\\) and the matrix \\(\\mathbf{A}\\) where each entry in the matrix is denoted as \\(a_{i,j}\\) (e.g. the entry in the first row and second column would be \\(a_{1,2}\\)). Your answer will be in the form of a matrix expressing result of the product \\(\\mathbf{Ax}\\).\n\nNote: please write your answers here in LaTeX\nANSWER\n\n\n\n[14 points] Matrix manipulations and multiplication. Machine learning involves working with many matrices, so this exercise will provide you with the opportunity to practice those skills.\nLet \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} -1 \\\\ 3 \\\\ 8 \\end{bmatrix}\\), \\(\\mathbf{c} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 6 \\end{bmatrix}\\), and \\(\\mathbf{I} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\nCompute the following using Python or indicate that it cannot be computed. Refer to NumPy’s tools for handling matrices. While all answers should be computer using Python, your response to whether each item can be computed should refer to underlying linear algebra. There may be circumstances when Python will produce an output, but based on the dimensions of the matrices involved, the linear algebra operation is not possible. For the case when an operation is invalid, explain why it is not.\nWhen the quantity can be computed, please provide both the Python code AND the output of that code (this need not be in LaTex)\n\n\\(\\mathbf{A}\\mathbf{A}\\)\n\\(\\mathbf{A}\\mathbf{A}^T\\)\n\\(\\mathbf{A}\\mathbf{b}\\)\n\\(\\mathbf{A}\\mathbf{b}^T\\)\n\\(\\mathbf{b}\\mathbf{A}\\)\n\\(\\mathbf{b}^T\\mathbf{A}\\)\n\\(\\mathbf{b}\\mathbf{b}\\)\n\\(\\mathbf{b}^T\\mathbf{b}\\)\n\\(\\mathbf{b}\\mathbf{b}^T\\)\n\\(\\mathbf{b} + \\mathbf{c}^T\\)\n\\(\\mathbf{b}^T\\mathbf{b}^T\\)\n\\(\\mathbf{A}^{-1}\\mathbf{b}\\)\n\\(\\mathbf{A}\\circ\\mathbf{A}\\)\n\\(\\mathbf{b}\\circ\\mathbf{c}\\)\n\nNote: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol “\\(\\circ\\)”.\nANSWER\n\n\n\n\n[8 points] Eigenvectors and eigenvalues. Eigenvectors and eigenvalues are useful for some machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning and in this course we will encounter them in relation to Principal Components Analysis (PCA), clustering algorithms, For an intuitive review of these concepts, explore this interactive website at Setosa.io. Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube here. For these questions, numpy may once again be helpful.\n\nCalculate the eigenvalues and corresponding eigenvectors of matrix \\(\\mathbf{A}\\) above, from the last question.\nChoose one of the eigenvector/eigenvalue pairs, \\(\\mathbf{v}\\) and \\(\\lambda\\), and show that \\(\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\\). This relationship extends to higher orders: \\(\\mathbf{A} \\mathbf{A} \\mathbf{v} = \\lambda^2 \\mathbf{v}\\)\nShow that the eigenvectors are orthogonal to one another (e.g. their inner product is zero). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. We will see this property is used in PCA to identify the dimensions of greatest variation in our data when we discuss dimensionality reduction.\n\nANSWER\n\n\n\n\n\n\n\n[10 points] Loading data and gathering insights from a real dataset\nIn data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\nData. The data for this problem can be found in the data subfolder in the assignments folder on github. The filename is a1_egrid2016.xlsx. This dataset is the Environmental Protection Agency’s (EPA) Emissions & Generation Resource Integrated Database (eGRID) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We’ll be using a subset of those data.\nThe fields we’ll be using include:\n\n\n\nfield\ndescription\n\n\n\n\nSEQPLT16\neGRID2016 Plant file sequence number (the index)\n\n\nPSTATABB\nPlant state abbreviation\n\n\nPNAME\nPlant name\n\n\nLAT\nPlant latitude\n\n\nLON\nPlant longitude\n\n\nPLPRMFL\nPlant primary fuel\n\n\nCAPFAC\nPlant capacity factor\n\n\nNAMEPCAP\nPlant nameplate capacity (Megawatts MW)\n\n\nPLNGENAN\nPlant annual net generation (Megawatt-hours MWh)\n\n\nPLCO2EQA\nPlant annual CO2 equivalent emissions (tons)\n\n\n\nFor more details on the data, you can refer to the eGrid technical documents. For example, you may want to review page 45 and the section “Plant Primary Fuel (PLPRMFL)”, which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\nThere also are a couple of “gotchas” to watch out for with this dataset: - The headers are on the second row and you’ll want to ignore the first row (they’re more detailed descriptions of the headers). - NaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with these sorts of missing values will be important.\nYour objective. For this dataset, your goal is to answer the following questions about electricity generation in the United States:\n(a) Which plant has generated the most energy (measured in MWh)?\n(b) What is the name of the northern-most power plant in the United States?\n(c) What is the state where the northern-most power plant in the United States is located?\n(d) Plot a bar plot showing the amount of energy produced by each fuel type across all plants.\n(e) From the plot in (d), which fuel for generation produces the most energy (MWh) in the United States?\nANSWER\n\n\n\n\n[6 points] Vectorization. When we first learn to code and think about iterating over an array, we often use loops. If implemented correctly, that does the trick. In machine learning, we iterate over so much data that those loops can lead to significant slow downs if they are not computationally efficient. In Python, vectorizing code and relying on matrix operations with efficient tools like numpy is typically the faster approach. Of course, numpy relies on loops to complete the computation, but this is at a lower level of programming (typically in C), and therefore is much more efficient. This exercise will explore the benefits of vectorization. Since many machine learning techniques rely on matrix operations, it’s helpful to begin thinking about implementing algorithms using vector forms.\nBegin by creating an array of 10 million random numbers using the numpy random.randn module. Compute the sum of the squares of those random numbers first in a for loop, then using Numpy’s dot module to perform an inner (dot) product. Verify that your code produces the same output in each case. Time how long it takes to compute each and report the results and report the output. How many times faster is the vectorized code than the for loop approach? (Note - your results may vary from run to run).\nYour output should use the print() function as follows (where the # symbols represent your answers, to a reasonable precision of 4-5 significant figures):\nTime [sec] (non-vectorized): ######\nTime [sec] (vectorized):     ######\nThe vectorized code is ##### times faster than the nonvectorized code\nANSWER\n\n\n\n\n[10 points] This exercise will walk through some basic numerical programming and probabilistic thinking exercises, two skills which are frequently used in machine learning for answering questions from our data. 1. Synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=2\\) and a standard deviation of \\(\\sigma=1\\). Call these observations from a random variable \\(X\\), and call the vector of observations that you generate, \\(\\textbf{x}\\). 2. Calculate the mean and standard deviation of \\(\\textbf{x}\\) to validate (1) and provide the result to a precision of four significant figures. 3. Plot a histogram of the data in \\(\\textbf{x}\\) with 30 bins 4. What is the 90th percentile of \\(\\textbf{x}\\)? The 90th percentile is the value below which 90% of observations can be found. 5. What is the 99th percentile of \\(\\textbf{x}\\)? 6. Now synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=0\\) and a standard deviation of \\(\\sigma=3\\). Call these observations from a random variable \\(Y\\), and call the vector of observations that you generate, \\(\\textbf{y}\\). 7. Create a new figure and plot the histogram of the data in \\(\\textbf{y}\\) on the same axes with the histogram of \\(\\textbf{x}\\), so that both histograms can be seen and compared. 8. Using the observations from \\(\\textbf{x}\\) and \\(\\textbf{y}\\), estimate \\(E[XY]\\)\nANSWER\n\n\n\n\n\n\n\n[4 points] Git is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the course website. As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\nComplete the Atlassian Git tutorial, specifically the following listed sections. Try each concept that’s presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as Github or Duke’s Gitlab. 1. What is version control 2. What is Git 3. Install Git 4. Setting up a repository 5. Saving changes 6. Inspecting a repository 7. Undoing changes 8. Rewriting history 9. Syncing 10. Making a pull request 11. Using branches 12. Comparing workflows\nI also have created two videos on the topic to help you understand some of these concepts: Git basics and a step-by-step tutorial.\nAs an additional resource, Microsoft now offers a git tutorial on this topic as well.\nFor your answer, affirm that you either completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets.\nANSWER\nI, [your name here], affirm that I have [completed the above tutorial / I have previous experience that covers all the content in this tutorial]\n\n\n\n\n\n\n\n[15 points] Here you’ll bring together some of the individual skills that you demonstrated above and create a Jupyter notebook based blog post on your exploratory data analysis. Your goal is to identify a question or problem and to work towards solving it or providing additional information or evidence (data) related to it through your data analysis. Below, we walk through a process to follow for your analysis. Additionally, you can find an example of a well-done exploratory data analysis here from past years.\n\nFind a dataset that interests you and relates to a question or problem that you find intriguing.\nDescribe the dataset, the source of the data, and the reason the dataset was of interest. Include a description of the features, data size, data creator and year of creation (if available), etc. What question are you hoping to answer through exploring the dataset?\nCheck the data and see if they need to be cleaned: are there missing values? Are there clearly erroneous values? Do two tables need to be merged together? Clean the data so it can be visualized. If the data are clean, state how you know they are clean (what did you check?).\nPlot the data, demonstrating interesting features that you discover. Are there any relationships between variables that were surprising or patterns that emerged? Please exercise creativity and curiosity in your plots. You should have at least 4 plots exploring the data in different ways.\nWhat insights are you able to take away from exploring the data? Is there a reason why analyzing the dataset you chose is particularly interesting or important? Summarize this for a general audience (imagine your publishing a blog post online) - boil down your findings in a way that is accessible, but still accurate.\n\nHere your analysis will evaluated based on: 1. Motivation: was the purpose of the choice of data clearly articulated? Why was the dataset chosen and what was the goal of the analysis? 2. Data cleaning: were any issues with the data investigated and, if found, were they resolved? 3. Quality of data exploration: were at least 4 unique plots (minimum) included and did those plots demonstrate interesting aspects of the data? Was there a clear purpose and takeaway from EACH plot? 4. Interpretation: Were the insights revealed through the analysis and their potential implications clearly explained? Was there an overall conclusion to the analysis?\nANSWER\n\n\nSource: Assignment_1.ipynb"
  },
  {
    "objectID": "assignments/assignment1.html#your-full-name-here",
    "href": "assignments/assignment1.html#your-full-name-here",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "Netid: Your netid here\nNames of students you worked with on this assignment: LIST HERE IF APPLICABLE (delete if not)\nNote: this assignment falls under collaboration Mode 2: Individual Assignment – Collaboration Permitted. Please refer to the syllabus for additional information.\nInstructions for all assignments can be found here, and are also linked to from the course syllabus.\nTotal points in the assignment add up to 90; an additional 10 points are allocated to professionalism and presentation quality."
  },
  {
    "objectID": "assignments/assignment1.html#section",
    "href": "assignments/assignment1.html#section",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[3 points]\nLet \\(f(x) = \\begin{cases}  0 & x &lt; 0 \\\\  \\alpha x^2 & 0 \\leq x \\leq 2 \\\\  0 & 2 &lt; x  \\end{cases}\\)\nFor what value of \\(\\alpha\\) is \\(f(x)\\) a valid probability density function?\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-1",
    "href": "assignments/assignment1.html#section-1",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[3 points] What is the cumulative distribution function (CDF) that corresponds to the following probability distribution function? Please state the value of the CDF for all possible values of \\(x\\).\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-2",
    "href": "assignments/assignment1.html#section-2",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[6 points] For the probability distribution function for the random variable \\(X\\),\n\\(f(x) = \\begin{cases}  \\frac{1}{3} & 0 &lt; x &lt; 3 \\\\  0 & \\text{otherwise}  \\end{cases}\\)\nwhat is the (a) expected value and (b) variance of \\(X\\). Show all work.\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-3",
    "href": "assignments/assignment1.html#section-3",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[6 points] You are given the training data below and asked to determine the probability that a sample of \\(x=0.54\\) comes from class 1, or equivalently, \\(P(Y=1|X=0.54)\\). The feature,, \\(x\\) can take on values between 0 and 1.\n\n\n\n\n\n\n\n\n\\(x\\) value range\nNegative data in the range (\\(x\\),\\(y=0\\))\nPositive data in the range (\\(x\\),\\(y=1\\))\n\n\n\n\n0.0 - 0.1\n(0.05,0),(0.07,0)\nNone\n\n\n0.1 - 0.2\n(0.11,0),(0.13,0),(0.19,0)\n(0.14,1)\n\n\n0.2 - 0.3\n(0.23,0)\n(0.24,1)\n\n\n0.3 - 0.4\n(0.35,0), (0.37,0)\n(0.32,1)\n\n\n0.4 - 0.5\n(0.49,0)\n(0.47,1)\n\n\n0.5 - 0.6\n(0.51,0)\n(0.53,1)\n\n\n0.6 - 0.7\nNone\n(0.61,1)\n\n\n0.7 - 0.8\nNone\n(0.77,1)\n\n\n0.8 - 0.9\nNone\n(0.83,1)\n\n\n0.9 - 1.0\nNone\n(0.92,1),(0.98,1)\n\n\n\nNote: You don’t need to use these data directly, but this provides an example of how the data could be distributed to form the empirical likelihoods and priors shown below.\nRecall Bayes’ Rule which computes the poster distribution, \\(P(Y|X)\\), based on the likelihood of the data conditioned on the class of the samples, \\(P(X|Y=y)\\), the prior \\(P(Y)\\) which is essentially the distribution of the class labels across all the data, and the evidence \\(P(X)\\) which is the distribution of the features, \\(X\\), regardless of class labels.\n\\[\\begin{equation}\nP(Y|X)= \\frac{P(X|Y)P(Y)}{P(X)}\n\\end{equation}\\]\nAlso, note that P(X) can alternatively be computed in this case as:\n\\[\\begin{equation}\nP(X) = P(X|Y=0)P(Y=0) + P(X|Y=1)P(Y=1)\n\\end{equation}\\]\nBelow are the prior and the likelihood functions based on the dataset above:\n\n\n\nWhat is \\(P(X=0.54|Y=1)P(Y=1)\\)?\nWhat is \\(P(X=0.54)\\)?\nWhat is \\(P(Y=1|X=0.54)\\)?\n\nShow each value you use for each computation.\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-5",
    "href": "assignments/assignment1.html#section-5",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[14 points] Matrix manipulations and multiplication. Machine learning involves working with many matrices, so this exercise will provide you with the opportunity to practice those skills.\nLet \\(\\mathbf{A} = \\begin{bmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 5 \\\\ 3 & 5 & 6 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} -1 \\\\ 3 \\\\ 8 \\end{bmatrix}\\), \\(\\mathbf{c} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 6 \\end{bmatrix}\\), and \\(\\mathbf{I} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\nCompute the following using Python or indicate that it cannot be computed. Refer to NumPy’s tools for handling matrices. While all answers should be computer using Python, your response to whether each item can be computed should refer to underlying linear algebra. There may be circumstances when Python will produce an output, but based on the dimensions of the matrices involved, the linear algebra operation is not possible. For the case when an operation is invalid, explain why it is not.\nWhen the quantity can be computed, please provide both the Python code AND the output of that code (this need not be in LaTex)\n\n\\(\\mathbf{A}\\mathbf{A}\\)\n\\(\\mathbf{A}\\mathbf{A}^T\\)\n\\(\\mathbf{A}\\mathbf{b}\\)\n\\(\\mathbf{A}\\mathbf{b}^T\\)\n\\(\\mathbf{b}\\mathbf{A}\\)\n\\(\\mathbf{b}^T\\mathbf{A}\\)\n\\(\\mathbf{b}\\mathbf{b}\\)\n\\(\\mathbf{b}^T\\mathbf{b}\\)\n\\(\\mathbf{b}\\mathbf{b}^T\\)\n\\(\\mathbf{b} + \\mathbf{c}^T\\)\n\\(\\mathbf{b}^T\\mathbf{b}^T\\)\n\\(\\mathbf{A}^{-1}\\mathbf{b}\\)\n\\(\\mathbf{A}\\circ\\mathbf{A}\\)\n\\(\\mathbf{b}\\circ\\mathbf{c}\\)\n\nNote: The element-wise (or Hadamard) product is the product of each element in one matrix with the corresponding element in another matrix, and is represented by the symbol “\\(\\circ\\)”.\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-6",
    "href": "assignments/assignment1.html#section-6",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[8 points] Eigenvectors and eigenvalues. Eigenvectors and eigenvalues are useful for some machine learning algorithms, but the concepts take time to solidly grasp. They are used extensively in machine learning and in this course we will encounter them in relation to Principal Components Analysis (PCA), clustering algorithms, For an intuitive review of these concepts, explore this interactive website at Setosa.io. Also, the series of linear algebra videos by Grant Sanderson of 3Brown1Blue are excellent and can be viewed on youtube here. For these questions, numpy may once again be helpful.\n\nCalculate the eigenvalues and corresponding eigenvectors of matrix \\(\\mathbf{A}\\) above, from the last question.\nChoose one of the eigenvector/eigenvalue pairs, \\(\\mathbf{v}\\) and \\(\\lambda\\), and show that \\(\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}\\). This relationship extends to higher orders: \\(\\mathbf{A} \\mathbf{A} \\mathbf{v} = \\lambda^2 \\mathbf{v}\\)\nShow that the eigenvectors are orthogonal to one another (e.g. their inner product is zero). This is true for eigenvectors from real, symmetric matrices. In three dimensions or less, this means that the eigenvectors are perpendicular to each other. Typically we use the orthogonal basis of our standard x, y, and z, Cartesian coordinates, which allows us, if we combine them linearly, to represent any point in a 3D space. But any three orthogonal vectors can do the same. We will see this property is used in PCA to identify the dimensions of greatest variation in our data when we discuss dimensionality reduction.\n\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-7",
    "href": "assignments/assignment1.html#section-7",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[10 points] Loading data and gathering insights from a real dataset\nIn data science, we often need to have a sense of the idiosyncrasies of the data, how they relate to the questions we are trying to answer, and to use that information to help us to determine what approach, such as machine learning, we may need to apply to achieve our goal. This exercise provides practice in exploring a dataset and answering question that might arise from applications related to the data.\nData. The data for this problem can be found in the data subfolder in the assignments folder on github. The filename is a1_egrid2016.xlsx. This dataset is the Environmental Protection Agency’s (EPA) Emissions & Generation Resource Integrated Database (eGRID) containing information about all power plants in the United States, the amount of generation they produce, what fuel they use, the location of the plant, and many more quantities. We’ll be using a subset of those data.\nThe fields we’ll be using include:\n\n\n\nfield\ndescription\n\n\n\n\nSEQPLT16\neGRID2016 Plant file sequence number (the index)\n\n\nPSTATABB\nPlant state abbreviation\n\n\nPNAME\nPlant name\n\n\nLAT\nPlant latitude\n\n\nLON\nPlant longitude\n\n\nPLPRMFL\nPlant primary fuel\n\n\nCAPFAC\nPlant capacity factor\n\n\nNAMEPCAP\nPlant nameplate capacity (Megawatts MW)\n\n\nPLNGENAN\nPlant annual net generation (Megawatt-hours MWh)\n\n\nPLCO2EQA\nPlant annual CO2 equivalent emissions (tons)\n\n\n\nFor more details on the data, you can refer to the eGrid technical documents. For example, you may want to review page 45 and the section “Plant Primary Fuel (PLPRMFL)”, which gives the full names of the fuel types including WND for wind, NG for natural gas, BIT for Bituminous coal, etc.\nThere also are a couple of “gotchas” to watch out for with this dataset: - The headers are on the second row and you’ll want to ignore the first row (they’re more detailed descriptions of the headers). - NaN values represent blanks in the data. These will appear regularly in real-world data, so getting experience working with these sorts of missing values will be important.\nYour objective. For this dataset, your goal is to answer the following questions about electricity generation in the United States:\n(a) Which plant has generated the most energy (measured in MWh)?\n(b) What is the name of the northern-most power plant in the United States?\n(c) What is the state where the northern-most power plant in the United States is located?\n(d) Plot a bar plot showing the amount of energy produced by each fuel type across all plants.\n(e) From the plot in (d), which fuel for generation produces the most energy (MWh) in the United States?\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-8",
    "href": "assignments/assignment1.html#section-8",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[6 points] Vectorization. When we first learn to code and think about iterating over an array, we often use loops. If implemented correctly, that does the trick. In machine learning, we iterate over so much data that those loops can lead to significant slow downs if they are not computationally efficient. In Python, vectorizing code and relying on matrix operations with efficient tools like numpy is typically the faster approach. Of course, numpy relies on loops to complete the computation, but this is at a lower level of programming (typically in C), and therefore is much more efficient. This exercise will explore the benefits of vectorization. Since many machine learning techniques rely on matrix operations, it’s helpful to begin thinking about implementing algorithms using vector forms.\nBegin by creating an array of 10 million random numbers using the numpy random.randn module. Compute the sum of the squares of those random numbers first in a for loop, then using Numpy’s dot module to perform an inner (dot) product. Verify that your code produces the same output in each case. Time how long it takes to compute each and report the results and report the output. How many times faster is the vectorized code than the for loop approach? (Note - your results may vary from run to run).\nYour output should use the print() function as follows (where the # symbols represent your answers, to a reasonable precision of 4-5 significant figures):\nTime [sec] (non-vectorized): ######\nTime [sec] (vectorized):     ######\nThe vectorized code is ##### times faster than the nonvectorized code\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-9",
    "href": "assignments/assignment1.html#section-9",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[10 points] This exercise will walk through some basic numerical programming and probabilistic thinking exercises, two skills which are frequently used in machine learning for answering questions from our data. 1. Synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=2\\) and a standard deviation of \\(\\sigma=1\\). Call these observations from a random variable \\(X\\), and call the vector of observations that you generate, \\(\\textbf{x}\\). 2. Calculate the mean and standard deviation of \\(\\textbf{x}\\) to validate (1) and provide the result to a precision of four significant figures. 3. Plot a histogram of the data in \\(\\textbf{x}\\) with 30 bins 4. What is the 90th percentile of \\(\\textbf{x}\\)? The 90th percentile is the value below which 90% of observations can be found. 5. What is the 99th percentile of \\(\\textbf{x}\\)? 6. Now synthesize \\(n=10^4\\) normally distributed data points with mean \\(\\mu=0\\) and a standard deviation of \\(\\sigma=3\\). Call these observations from a random variable \\(Y\\), and call the vector of observations that you generate, \\(\\textbf{y}\\). 7. Create a new figure and plot the histogram of the data in \\(\\textbf{y}\\) on the same axes with the histogram of \\(\\textbf{x}\\), so that both histograms can be seen and compared. 8. Using the observations from \\(\\textbf{x}\\) and \\(\\textbf{y}\\), estimate \\(E[XY]\\)\nANSWER"
  },
  {
    "objectID": "assignments/assignment1.html#section-10",
    "href": "assignments/assignment1.html#section-10",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[4 points] Git is efficient for collaboration, and expectation in industry, and one of the best ways to share results in academia. You can even use some Git repositories (e.g. Github) as hosts for website, such as with the course website. As a data scientist with experience in machine learning, Git is expected. We will interact with Git repositories (a.k.a. repos) throughout this course, and your project will require the use of git repos for collaboration.\nComplete the Atlassian Git tutorial, specifically the following listed sections. Try each concept that’s presented. For this tutorial, instead of using BitBucket as your remote repository host, you may use your preferred platform such as Github or Duke’s Gitlab. 1. What is version control 2. What is Git 3. Install Git 4. Setting up a repository 5. Saving changes 6. Inspecting a repository 7. Undoing changes 8. Rewriting history 9. Syncing 10. Making a pull request 11. Using branches 12. Comparing workflows\nI also have created two videos on the topic to help you understand some of these concepts: Git basics and a step-by-step tutorial.\nAs an additional resource, Microsoft now offers a git tutorial on this topic as well.\nFor your answer, affirm that you either completed the tutorials above OR have previous experience with ALL of the concepts above. Confirm this by typing your name below and selecting the situation that applies from the two options in brackets.\nANSWER\nI, [your name here], affirm that I have [completed the above tutorial / I have previous experience that covers all the content in this tutorial]"
  },
  {
    "objectID": "assignments/assignment1.html#section-11",
    "href": "assignments/assignment1.html#section-11",
    "title": "Assignment 1 - Probability, Linear Algebra, & Computational Programming",
    "section": "",
    "text": "[15 points] Here you’ll bring together some of the individual skills that you demonstrated above and create a Jupyter notebook based blog post on your exploratory data analysis. Your goal is to identify a question or problem and to work towards solving it or providing additional information or evidence (data) related to it through your data analysis. Below, we walk through a process to follow for your analysis. Additionally, you can find an example of a well-done exploratory data analysis here from past years.\n\nFind a dataset that interests you and relates to a question or problem that you find intriguing.\nDescribe the dataset, the source of the data, and the reason the dataset was of interest. Include a description of the features, data size, data creator and year of creation (if available), etc. What question are you hoping to answer through exploring the dataset?\nCheck the data and see if they need to be cleaned: are there missing values? Are there clearly erroneous values? Do two tables need to be merged together? Clean the data so it can be visualized. If the data are clean, state how you know they are clean (what did you check?).\nPlot the data, demonstrating interesting features that you discover. Are there any relationships between variables that were surprising or patterns that emerged? Please exercise creativity and curiosity in your plots. You should have at least 4 plots exploring the data in different ways.\nWhat insights are you able to take away from exploring the data? Is there a reason why analyzing the dataset you chose is particularly interesting or important? Summarize this for a general audience (imagine your publishing a blog post online) - boil down your findings in a way that is accessible, but still accurate.\n\nHere your analysis will evaluated based on: 1. Motivation: was the purpose of the choice of data clearly articulated? Why was the dataset chosen and what was the goal of the analysis? 2. Data cleaning: were any issues with the data investigated and, if found, were they resolved? 3. Quality of data exploration: were at least 4 unique plots (minimum) included and did those plots demonstrate interesting aspects of the data? Was there a clear purpose and takeaway from EACH plot? 4. Interpretation: Were the insights revealed through the analysis and their potential implications clearly explained? Was there an overall conclusion to the analysis?\nANSWER"
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Software and Hardware Tools",
    "section": "",
    "text": "Programming language: Python\nWe will use Python 3.x. The Anaconda distribution is recommended and comes with the most common packages. Python continues to be an one of the top programming languages and the rich packages in the language make it an excellent choice for machine learning. In particular the Python ecosystem of packages makes it a natural choice for ML including core numerical programming and plotting libraries like numpy, scipy, matplotlib, and pandas as well as excellent packages for machine learning algorithm development and statistical modeling including Scikit-Learn, Keras, and Pytorch.\n\n\nDevelopment environments: VS Code and Jupyter Notebooks\nJupyter lab or Jupyter notebook will be appropriate for most class assignments. We highly encourage you to use Visual Studio Code, in particular due to the debugging capabilities. There are many configurations that may work for you, but I would recommend begin by gathering ideas in Jupyter Notebooks. Once you have the basic structure of your code worked out, consider moving it to a .py file to make it easier and cleaner to run and build on.\n\n\nGraphics processing units (GPUs)\nGPUs are the workhorses of many modern machine learning algorithms, especially any that involve neural network-based architectures. There will be a small number of assignments that will require additional computation from that of GPUs. For these, we will use Google Colab, which is a free notebook environment that enables access to cloud resources including GPUs. For longer sessions before timeouts, greater RAM, and better GPUs you can optionally upgrade to Colab Pro.\nWe will also be making a limited number of cloud credits available to students later in the semester."
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Instructional Team",
    "section": "",
    "text": "Contact Us & Office Hourse\n\n\n\nReach out to us through Ed Discussions for any questions. Office hours are also posted on Ed.\n\n\n\nInstructor\n\n\n\nKyle Bradbury (, )\n\n\nDr. Kyle Bradbury develops and applies machine learning techniques to better understand and manage energy and climate resources. His work focuses on how to advance machine learning methodologies and apply them to solve energy and climate system challenges through the development of open source, widely-applicable computational tools. His current research interests include developing scalable computer vision techniques for assessing energy resources, infrastructure, and access globally through the use of publicly available remote sensing data. Methodologically, he investigates how to overcome the challenge of distribution shift in computer vision through novel training paradigms that require less labeled data.\n\n\nTeaching Assistants\n\n\n\n\n\n\nSusanna Anil (, )\n\n\n\n\n\n\n\nDingkun Yang (, )\n\n\n\n\n\n\n\nYuanjing Zhu (, )"
  },
  {
    "objectID": "pedagogy.html",
    "href": "pedagogy.html",
    "title": "Pedagogy",
    "section": "",
    "text": "Tenet #1: Good learning is active learning\nEveryone who was good at something was once bad at it. Learning comes from practice. No amount of reading or video/lecture watching alone will help you to become good without actively engaging with the material through practice. That is why this entire course is focused on supporting you to actively apply machine learning techniques through the assignments, quizzes, and project. Concept described in Make It Stick.\n\n\nTenet #2: Desirable difficulty leads to meaningful learning\nLearning is most effective when there’s a degree of struggle with the material. “Requiring students to organize new information and to work harder in the initial learning period can lead to greater and deeper learning. Although this struggle, dubbed a desirable difficulty…may at first be frustrating to learner and teacher alike, ultimately it improves long-term retention” (Excerpt from A Concise Guide to Improving Student Learning: Six Evidence-Based Principles and How to Apply Them). Desirable difficulties help you build connections between concepts and learn representations of knowledge (meta-cognition) that, like an index of a book, will increase your ability to creatively connect concepts and think more deeply about the topic. This is also described in Make It Stick.\n\n\nTenet #3: Read, reflect, recall is a pattern for effective learning\nSpaced retrieval and reflection is a key to effective learning. When we learn something, if we don’t use it, the knowledge fades. However, if we return to the material, apply it, create with it, we’re increasing the probability of long-term learning. This is why you will interact with each concept typically 4 times: lectures, readings, quizzes, and assignments, and at least one more time for those concepts involved in the final project. An added benefit of the frequent reflection through quizzes is that it tests your knowledge regularly, helping us to avoid the illusion of knowledge (thinking we know something, when we actually do not).\nReference Brown, P.C., Roediger III, H.L. and McDaniel, M.A., 2014. Make it stick: The science of successful learning. Harvard University Press."
  },
  {
    "objectID": "notebooks/assignment_instructions.html",
    "href": "notebooks/assignment_instructions.html",
    "title": "Assignment Instructions",
    "section": "",
    "text": "Place your answer in the indicated place in the document. Each question is followed by one or more sections, each with an empty cell called “ANSWER”. Insert your answer for that question below the word “ANSWER”. You may use multiple cells for each answer that include both markdown and code.\nEnsure that all cells have been run. Cells that have not been run will be treated as unanswered questions and assigned zero points.\nCreate a PDF document of your notebook. There are a few ways to do this. Please see the guide below.\nCheck that your content is easily legible prior to submission. Look over your PDF before you submit it. If we cannot read it, or parts are missing, we cannot grade it, and no credit will be given for anything we cannot read.\nSubmit your pdf by the deadline on gradescope. Any assignments received after the deadline will be treated as late. Please see this video for how to submit your assignment on gradescope. The submission link is here: https://www.gradescope.com/\n\nSee below for a checklist for your document before you submit."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#how-to-submit-assignments",
    "href": "notebooks/assignment_instructions.html#how-to-submit-assignments",
    "title": "Assignment Instructions",
    "section": "",
    "text": "Place your answer in the indicated place in the document. Each question is followed by one or more sections, each with an empty cell called “ANSWER”. Insert your answer for that question below the word “ANSWER”. You may use multiple cells for each answer that include both markdown and code.\nEnsure that all cells have been run. Cells that have not been run will be treated as unanswered questions and assigned zero points.\nCreate a PDF document of your notebook. There are a few ways to do this. Please see the guide below.\nCheck that your content is easily legible prior to submission. Look over your PDF before you submit it. If we cannot read it, or parts are missing, we cannot grade it, and no credit will be given for anything we cannot read.\nSubmit your pdf by the deadline on gradescope. Any assignments received after the deadline will be treated as late. Please see this video for how to submit your assignment on gradescope. The submission link is here: https://www.gradescope.com/\n\nSee below for a checklist for your document before you submit."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#jupyter-notebooks",
    "href": "notebooks/assignment_instructions.html#jupyter-notebooks",
    "title": "Assignment Instructions",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nIf you could use help getting started or a refresher on Jupyter notebooks, check out this video for more on basic Jupyter functionality. Using Jupyter notebooks allows you to practice applying machine learning concepts while building programming and writing skills, strengthening your ability to both create creative solutions to machine learning challenges while simultaneously enhancing your ability to communicate the meaning behind your findings and why others should give credence to your results."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#rendering-a-pdf-file",
    "href": "notebooks/assignment_instructions.html#rendering-a-pdf-file",
    "title": "Assignment Instructions",
    "section": "Rendering a PDF file",
    "text": "Rendering a PDF file\n\nOption 1: Render from VS Code (without additional extensions)\nWe recommend using VS Code for this course and for use in developing your Jupyter notebooks. Rendering pdfs from Jupyter Notebooks in VS Code has a couple of extra steps, though. To render a pdf without any additional extensions, you do the following:\n\nOpen your notebook in VS code and hit the ellipsis (the three horizontal dots) just to the right of the tools for your notebook:\n\n\n\nClick “Export”:\n\n\n\nSelect “HTML” (exporting to pdf won’t work without the installation of additional tools):\n\n\n\nOpen the HTML file you just saved in Chrome:\n\n\n\nClick the options button (three vertical dots) and click “print”:\n\n\n\nSelect the destination as “Save as PDF” and click “Save”:\n\n\n\nVoila! You should have a pdf\n\n\n\nOption 2: Render from VS Code with Quarto and LaTeX\nYou can save a lot of the above steps if you’re willing to install 2 things: 1. Install Quarto, which is an open-source scientific and technical publishing system (great for making websites and blogs for your professional portfolio). 2. Install the Quarto VS Code extension 3. Install a version of tex for your operating system. 4. Once you do this, you can directly export your documents to pdf:\n\n\n\nOption 3: Render from Jupyter Notebooks\nOpen your notebook in a Jupyter Notebook in Google Chrome. Go to File-&gt;Print Preview, then after verifying the document looks correct, click “print” and for your printer choose “Save as PDF.”\nWhichever method you choose - always check your pdf before submitting to make sure everything is rendered and nothing is cut off!"
  },
  {
    "objectID": "notebooks/assignment_instructions.html#assignment-grading",
    "href": "notebooks/assignment_instructions.html#assignment-grading",
    "title": "Assignment Instructions",
    "section": "Assignment Grading",
    "text": "Assignment Grading\nAssignment grading will consist of two components: content (90%) and presentation (10%). The content grade will be based on the accuracy and completeness (including the depth) of your answers to each question. If a question asks you to explain, hypothesize, or otherwise think critically, be sure to demonstrate your critical engagement with the question. The presentation score will be based on how well you communicate in writing, figure creation, and coding. Clear writing (with appropriate grammar and spelling), organized answers, well-organized/commented code, and properly formatted figures will ensure your success on the presentation component of the assignment."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#example-question-and-response",
    "href": "notebooks/assignment_instructions.html#example-question-and-response",
    "title": "Assignment Instructions",
    "section": "Example Question and Response",
    "text": "Example Question and Response\nBelow is an example question to demonstrate how to answer a question using a Jupyter notebook.\nCalculate the (a) first and (b) second derivative of \\(f(t) = t^3\\). lastly, (c) numerically evaluate the second derivative for \\(t=3\\). If \\(t\\) represents time (seconds) and \\(f\\) represents distance of a ball over time (in meters), then (d) what does the second derivative at \\(t=3\\) mean?\nANSWER\nThe first and second derivatives are calculated as follows:\n(a) The first derivative is: \\[ \\frac{df}{dt} = 3t^2\\]\n(b) The second derivative is: \\[ \\frac{d^2f}{dt^2} = 6t\\]\n(c) We calculate the second derivative at time \\(t=3\\) as follows:\n\n# (c) Numerically evaluate the second derivative for f(x)\n\n# Initialize variables for the analysis\nt = 3\n\n# Compute the derivative\ndf2 = 6 * t\n\nprint('(c) The second derivative at t = 3 is {}'.format(df2))\n\n(c) The second derivative at t = 3 is 18\n\n\n(d) This derivative, which is the second derivative of distance with respect to time represents the acceleration of the ball in \\(m^2\\) per second. This shows how quickly the ball is increasing its speed over time.\n\nFigure Example:\nHere is an example of a well-prepared figure that checks all of the requirements for good figures. To demonstrate we’ll start by loading some data to plot (the example here is from the lecture on bias-variance tradeoff):\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Load the data for plotting\ndatafilename = './plot_example_data/data.pkl'\ninfile = open(datafilename,'rb')\nloaded_data = pickle.load(infile)\ninfile.close()\n\n# Store the loaded data in convenient variable names\nkvalues = loaded_data['kvalues']\nerror_training = loaded_data['error_training']\nerror_testing = loaded_data['error_testing']\nerror_bayesclf = loaded_data['error_bayesclf']\n\n# Choose colors that will be distinguishable from one another\ncolor0 = '#121619' # Dark grey\ncolor1 = '#00B050' # Green\ncolor2 = '#7c7c7c' # Light grey\n\nThen, we create the plot:\n\n%config InlineBackend.figure_format = 'retina' # Make clear on high-res screens\n\n# Create the plot\nplt.figure(figsize=(7,5), dpi= 100) # Adjust the figure size and dots per inch \nplt.semilogx(kvalues,error_training,color=color0,label='Training (in-sample)')\nplt.semilogx(kvalues,error_testing,color=color1,label='Test (out-of-sample)')\nplt.semilogx(kvalues,error_bayesclf,'--',color=color2,label='Bayes (optimal)')\nplt.legend()\nplt.grid('on')\nplt.xlabel('k nearest neighbors') # Always use X and Y labels\nplt.ylabel('Binary Classification Error Rate')\nplt.axis([1,200,0,0.5]) # Ensure the axis is the right size for the plot data\nplt.tight_layout() # Use this to maximize the use of space in the figure\nplt.show()\n\n\n\n\nNote the clear x and y axis labels and legends - there are no acronyms or shorthands used, just the full description of what is being plotted. Also note how easy it is to compare the color of lines and identify the baseline (Bayes) comparison line. Make your plots easy to read and understand."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#checklist-before-submitting-your-assignment",
    "href": "notebooks/assignment_instructions.html#checklist-before-submitting-your-assignment",
    "title": "Assignment Instructions",
    "section": "Checklist before submitting your assignment",
    "text": "Checklist before submitting your assignment\n\nAnswers are placed in the indicated place in the document. Each question is followed by one or more sections, each with an empty cell called “ANSWER”. Insert your answer for that question below the word “ANSWER”. You may use multiple cells for each answer that include both markdown and code. For full credit, submitted assignments should be easily navigable with answers for subquestions clearly indicated (e.g. using “(a)” to indicate a subquestion).\nAll cells have been run. Cells that have not been run will be treated as unanswered questions and assigned zero points.\nYour submission is a PDF document. See guidelines above\nYour content is legible prior to submission. Look over your PDF before you submit it. If we cannot read it, or parts are missing, we cannot grade it, and no credit will be given for anything we cannot read or is missing.\n\nCode is valid (able to run and producing the correct answer), neat, understandable, and well commented.\nMath is typeset using LateX equations.\nAll text that is not code should be formatted using markdown. Two references to help include: ref1 and ref2.\n\nDescriptive text provides adequate detail to answer questions. When questions as “why” or for you to describe findings, the expectation is that the response will be much more than one or two words.\nProper grammar and spelling are expected throughout\n\n\nFigure checklist\n\nAll plots should have axes labels and legible fonts (large enough to read).\nLegends are used when there are multiple series plotted on a single plot.\nFigures are crisp and clear - there are no blurry figures.\nMarkers on plots are properly sized (e.g. not be too big nor too small) and/or have the appropriate level of transparency to be able to clearly read the data without obscuring other data.\nIf there are multiple colors used on a plot, EVERY color can be distinguished from the rest."
  },
  {
    "objectID": "notebooks/assignment_instructions.html#sources",
    "href": "notebooks/assignment_instructions.html#sources",
    "title": "Assignment Instructions",
    "section": "Sources",
    "text": "Sources\nSome questions on the assignments are adapted from sources including:\n1. James et al., An Introduction to Statistical Learning\n2. Abu-Mostafa, Yaser, Learning from Data\n3. Weinberger, Kilian, Machine Learning CS4780, Cornell University"
  }
]