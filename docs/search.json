[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.\n\n\n\n\n\n\nKey to books used below\n\n\n\n\nISL = Introduction to Statistical Learning, by James, Witten, Hastie, and Tibshirani\nDM = Introduction to Data Mining, by Tan, Steinbach, Karpatne, and Kumar\nPRML = Pattern Recognition and Machine Learning, by Bishop\nDL = Deep Learning, by Goodfellow, Bengio, and Courville\nRL = Reinforcement Learning: An Introduction, by Sutton and Barto\n\n\n\n\n\n  \n    \n      Event Type\n      Date\n      Description\n      Readings\n      Course Materials\n    \n  \n\n  \n    Lecture 1\n    Thursday Jan 11\n    \n      What is machine learning? \n      Course overview and an orientation to the major branches of machine learning: supervised, unsupervised, and reinforcement learning \n    \n    None\n    \n      \n      [slides]\n    \n  \n\n  \n    \n    Monday  Jan 15\n    Martin Luther King Jr. Day\n    \n    \n  \n\n  \n    \n    \n    Module 1: Supervised Learning\n    \n    \n  \n\n  \n    Lecture 2\n    Tuesday  Jan 16\n    \n      An end-to-end machine learning example \n      An introduction to formulating a supervised machine learning problem. Stating the problem, creating the model, evaluating performance, and operationalizing the solution. \n    ISL Ch. 1 + 2.1Watch this lecture \n    \n      [slides]\n      \n      \n    \n  \n\n  \n    Lecture 3\n    Thursday  Jan 18\n    \n      How flexible should my algorithms be: the bias-variance tradeoff  \n      K-nearest neighbors classification and the bias-variance tradeoff \n    \n    ISL 2.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Monday Jan 22\n    Assignment #1 Due (at 9pm) Probability, Linear Algebra, & Computational Programming\n    \n    \n      \n      \n      \n      [submit]\n    \n  \n\n  \n    Lecture 4\n    Tuesday  Jan 23\n    \n      Linear Models I \n      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification\n    \n    ISL Intro of 3, 3.1, and 3.2\n    \n      [slides]\n      \n    \n  \n\n\n\n  \n    Lecture 5\n    Thursday  Jan 25\n    \n      Linear Models II \n      Nonlinear transformations of predictors; linear models for classification including the perceptron and logistic regression; cost/loss functions for classification (cross entropy loss); introduction to gradient descent.\n    \n    ISL 3.3 and 3.5\n    \n      [slides]\n      \n    \n  \n  \n  \n    Lecture 6\n    Tuesday  Jan 30\n    \n      Performance evaluation and model comparison \n      Choosing the right model: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, and confusion matrices\n    \n    ISL 4.1, 4.2, and 4.3\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 7\n    Thursday  Feb 1\n    \n      Resampling methods for performance evaluation: model validation  and testing strategies \n      How to use model performance metrics to measure metrics of generalization performance; resampling techniques: training, testing, and validation datasets and cross validation; common pitfalls around biased sampling and data snooping/leakage\n    \n    ISL 5.1 and 5.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     MondayFeb 5\n    Assignment #2 Due (at 9pm)Supervised Machine Learning Fundamentals\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 8\n    Tuesday  Feb 6\n    \n      Decision theory \n      A risk-based framework for determining to operate supervised learning algorithms in practice; choosing ROC operating points through risk-minimization and how application-specific costs associated with different types of errors can be used to determine optimal operating points for classifiers\n    \n    \n      Link to reading\n      \n      \n    \n    \n      [slides]\n      \n    \n  \n\n    \n    Lecture 9\n    Thursday  Feb 8\n    \n      Reducing overfit \n      Feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization; early stopping.\n    \n    ISL 6.1 and 6.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 10\n    Tuesday  Feb 13\n    \n      Generative models for classification \n      Generative vs discriminative models; linear discriminant analysis, quadratic discriminant analysis, and naïve Bayes\n    \n    ISL 4.4 and 4.5\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 11\n    Thursday  Feb 15\n    \n      Tree-based models and ensembles \n      From decision trees to random forests: bagging, bootstrapping, and boosting\n    \n    ISL 8.1 and 8.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Monday  Feb 19\n    Assignment #3 Due (at 9pm)Supervised learning model training and evaluation\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 12\n    Tuesday  Feb 20\n    \n      Kernel Methods \n      Introducing Kernel machines via the kernel perceptron, maximum margin classifiers, and support vector machines\n    \n    ISL Ch 9: 9.1-9.4\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 13\n    Thursday  Feb 22\n    \n      Neural networks I \n      Introduction to neural networks and representation learning; forward propagation, network architecture, and how to adapt to regression or classification problems\n    \n    PRML Ch 5: 5.1 \n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 14\n    Tuesday  Feb 27\n    \n      Neural networks II \n      Fitting a neural network to training data through gradient descent and backpropagation; how backpropagation is used to compute gradients in neural networks; hyperparameters and architecture choices in neural networks and practices for training neural networks warningfully\n    \n    PRML Ch 5: 5.3 (intro), 5.3.1, 5.3.2, and Calculus on Computational Graphs\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 15\n    Thursday  Feb 29\n    \n      Introduction to Deep learning \n      Common architectures of deep learning models, in particular convolutional neural networks for computer vision and the tools used to implement them\n    \n    DL Ch 11: Practical Methodology\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 2: Unsupervised Learning\n    \n    \n  \n\n  \n    Lecture 16\n    TuesdayMar 5\n    \n      Dimensionality reduction \n      The Curse of Dimensionality and intro to principal components analysis (PCA)\n    \n    ISL 6.3, 6.4, 12.1, and 12.2 \n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Monday  Mar 4\n    Assignment #4 Due (at 9pm)Neural Networks\n    \n    \n      [assignment] \n      [submit]\n      \n      \n      \n    \n  \n\n  \n    Lecture 17\n    ThursdayMar 7\n    \n      Principal components analysis (PCA) \n      Explaining how PCA works and how we calculate the principal components.\n    \n    ISL 12.4\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n     Friday  Mar 8 \n    Project Proposal Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    \n    Mar 9-16\n    Spring Break Week\n    \n    \n  \n\n  \n    Lecture 18\n    Tuesday  Mar 19\n    \n      Clustering I \n      From K-means to Gaussian mixture model clustering and Expectation Maximization\n    \n    DM Ch 7 (link): Intro, 7.1 and 7.2\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 19\n    Thursday  Mar 21\n    \n      Clustering II \n      Hierarchical clustering, DBSCAN, and spectral clustering\n    \n   DM Ch 7 (link): 7.3 and 7.4\n   \n     [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 3: Reinforcement Learning\n    \n    \n  \n\n  \n    Lecture 20\n    Tuesday  Mar 26\n    \n      Reinforcement Learning I \n      Formulating the reinforcement learning problem\n    \n    RL Ch 1: 1.1-1.6; Ch 2: 2.1-2.5\n    \n      [slides] \n      \n    \n  \n\n  \n    Lecture 21\n    Thursday  Mar 28\n    \n      Reinforcement Learning II \n      Policy and value functions, rewards, and introduction to Markov processes \n    \n    RL Ch 3\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Monday  Mar 25\n    Assignment #5 Due (at 9pm)  Kaggle Competition and Unsupervised LearningKaggle Competition Ends 9pm on Sun Mar 24\n    \n    \n      [assignment] \n      \n      [submit]\n      \n    \n  \n\n  \n    Lecture 22\n    Tuesday Apr 2\n    \n      Reinforcement Learning III \n      From Markov Chains to Markov Decision Processes (MDPs)\n    \n    RL Ch 4\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 3 \n    Draft Final Project Report Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    Lecture 23\n    Thursday  Apr 4\n    \n      Reinforcement Learning IV \n      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods\n    \n    RL Ch 5: 5.1-5.3\n    \n      [slides]\n      \n    \n  \n\n  \n    \n    \n    Module 4: Machine Learning Trends, Practical Considerations, and Advanced Topics\n    \n    \n  \n\n  \n    Lecture 24\n    Tuesday  Apr 11\n    \n      Advanced topics and applications I \n      A survey of advanced topics including semi- and self-supervised learning\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Lecture 25\n    Thursday  Apr 9\n    \n      Advanced topics and applications II \n      Discussion on where the field is heading and how to stay up-to-date\n    \n    None\n    \n      [slides]\n      \n    \n  \n\n  \n    Deliverable\n    Tuesday  Apr 16\n    \n      Final project showcase(last class meeting of the semester)\n    \n    \n    \n      [project]\n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 17 \n    Final Project Report Due (at 9pm)\n    \n    \n      [project] \n      [submit]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Thursday  Apr 18\n    Final Project Peer Evaluation Due (at 9pm)\n    \n    \n      [project]\n      \n      \n    \n  \n\n  \n    Deliverable\n    Wednesday  Apr 24\n    (Optional) Assignment #6 Due (at 9pm)Reinforcement learning\n    \n    \n      [assignment]"
  },
  {
    "objectID": "pedagogy.html",
    "href": "pedagogy.html",
    "title": "Pedagogy",
    "section": "",
    "text": "Tenet #1: Good learning is active learning\nEveryone who was good at something was once bad at it. Learning comes from practice. No amount of reading or video/lecture watching alone will help you to become good without actively engaging with the material through practice. That is why this entire course is focused on supporting you to actively apply machine learning techniques through the assignments, quizzes, and project. Concept described in Make It Stick.\n\n\nTenet #2: Desirable difficulty leads to meaningful learning\nLearning is most effective when there’s a degree of struggle with the material. “Requiring students to organize new information and to work harder in the initial learning period can lead to greater and deeper learning. Although this struggle, dubbed a desirable difficulty…may at first be frustrating to learner and teacher alike, ultimately it improves long-term retention” (Excerpt from A Concise Guide to Improving Student Learning: Six Evidence-Based Principles and How to Apply Them). Desirable difficulties help you build connections between concepts and learn representations of knowledge (meta-cognition) that, like an index of a book, will increase your ability to creatively connect concepts and think more deeply about the topic. This is also described in Make It Stick.\n\n\nTenet #3: Read, reflect, recall is a pattern for effective learning\nSpaced retrieval and reflection is a key to effective learning. When we learn something, if we don’t use it, the knowledge fades. However, if we return to the material, apply it, create with it, we’re increasing the probability of long-term learning. This is why you will interact with each concept typically 4 times: lectures, readings, quizzes, and assignments, and at least one more time for those concepts involved in the final project. An added benefit of the frequent reflection through quizzes is that it tests your knowledge regularly, helping us to avoid the illusion of knowledge (thinking we know something, when we actually do not)."
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Instructional Team",
    "section": "",
    "text": "Contact Us & Office Hourse\n\n\n\nReach out to us through Ed Discussions for any questions. Office hours are also posted on Ed.\n\n\n\nInstructor\n\n\n\nKyle Bradbury (, )\n\n\nDr. Kyle Bradbury develops and applies machine learning techniques to better understand and manage energy and climate resources. His work focuses on how to advance machine learning methodologies and apply them to solve energy and climate system challenges through the development of open source, widely-applicable computational tools. His current research interests include developing scalable computer vision techniques for assessing energy resources, infrastructure, and access globally through the use of publicly available remote sensing data. Methodologically, he investigates how to overcome the challenge of distribution shift in computer vision through novel training paradigms that require less labeled data.\n\n\nTeaching Assistants\n\n\n\n\n\n\nSusanna Anil (, )\n\n\n\n\n\n\n\nDingkun Yang (, )\n\n\n\n\n\n\n\nYuanjing Zhu (, )"
  },
  {
    "objectID": "index.html#course-summary",
    "href": "index.html#course-summary",
    "title": "Overview",
    "section": "Course Summary",
    "text": "Course Summary\nIn almost every field, there is a need to draw inference from or make decisions based on data. The goal of this course is to provide an introduction to machine learning that is approachable to diverse disciplines and empowers students to become proficient in the foundational concepts and tools. You will learn to (a) structure a machine learning problems and determine which algorithmic tools are appropriate, (b) evaluate the performance of your solution using field-appropriate metrics and practices, and (c) accurately interpret your model output and communicate your results to interdisciplinary audiences. This course is a fast-paced, applied introduction to machine learning that through extensive practice with foundational tools, helps you to develop your knowledge of foundational machine learning concepts, and provides practical experience with those tools to prepare you for practice or future study."
  },
  {
    "objectID": "index.html#detailed-description",
    "href": "index.html#detailed-description",
    "title": "Overview",
    "section": "Detailed description",
    "text": "Detailed description\nMachine learning is a collection of useful tools for understanding and making decisions based on data and past experience; it is not a hammer to be applied to every nail, but rather a precision tool to be used when needed. This course will begin with exploring the purpose of machine learning told through a discussion of the types of problems that machine learning can answer: describing, predicting, and strategizing based on data and the tools at our disposal to address these challenges: supervised learning including classification and regression; unsupervised learning including clustering and density estimation; and reinforcement learning. There will be a strong focus on how to formulate a machine learning problem. Central to that formulation will be developing an understanding of how to preprocess data for analysis (e.g. feature extraction/dimensionality reduction, training/validation data sampling), model selection, and performance evaluation with cross validation. The final topic of this course will be a brief overview of state-of-the-art machine learning techniques that are emerging in the field.\nThroughout this course, the focus will be on applying algorithms rather than diving deeply into theory. You will be asked to consider the practical issues of machine learning problem solving: challenges of applying machine learning code packages, striving for parsimony (simplicity of models) and interpretability, and ensuring model assumptions are valid for a given problem and dataset. This course will also stress the importance of team-based collaboration, the value of producing fully reproducible and validated results, and tools to help with both such as version control and code repositories.\nCommunicating your results. Data science solutions are only as impactful as the communicator who shares them: therefore communication of your findings will be a core component of this course. Demonstrating competency in data science means (a) exhibiting a working knowledge of technical concepts including programming, statistics, and mathematics and (b) being able to clearly communicate the problem you were trying to solve or question you were trying to answer, why it matters, and how well your analysis worked. You will have opportunities to practice these skills throughout this course in the context of interpreting and sharing the results of your analyses."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Logistics",
    "section": "",
    "text": "Class Time and Location\n\n\n\n\nWhen: Tuesday and Thursdays 10:15am - 11:30am\nWhere: French Science 2231\n\n\n\n\n\n\nEd Discussions: announcements, Q&A on course content (assignments, quizzes, grades), all course communications\nGradescope: Assignment & project submission & feedback\nCourse Schedule: Schedule & assignments\n\n\n\n\nEach book is available free online:\n\nAn Introduction to Statistical Learning with Python by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2013.\nPattern Recognition and Machine Learning by Christopher Bishop, 2006.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016.\nReinforcement Learning: An Introduction, by Richard Sutton and Andrew Barto, 2018.\n\n\n\n\nAssignments, projects, & quizzes: Assignments and projects details are posted on the course syllabus. For expectations and instructions on the assignments, see the Assignment Instructions. Quizzes are found on Sakai under “Tests & Quizzes”.\n\n\n\n55% Assignments (5, each worth ~11%)\n20% Quizzes (~23, each worth &lt;1%)\n25% Final Project\n\n\n\n\n\nThis course moves quickly, so having a firm grasp on prerequisites is important. The prerequisites are as follows:\n\nProgramming: Fundamentals of Python programming.\nMathematics: Calculus and linear algebra.\nStatistics: Introductory probability and statistics.\n\n\n\n\nProgramming language: We will use Python 3.x. The Anaconda distribution is recommended and comes with the most common packages. Python continues to be an one of the top programming languages and the rich packages in the language make it an excellent choice for machine learning. In particular the Python ecosystem of packages makes it a natural choice for ML including core numerical programming and plotting libraries like numpy, scipy, matplotlib, and pandas as well as excellent packages for machine learning algorithm development and statistical modeling including TensorFlow, Pytorch, Keras, Scikit-Learn.\nDevelopment environments: Jupyter lab or Jupyter notebook will be appropriate for most class assignments. We highly encourage you to use Visual Studio Code or Spyder are for larger projects, in particular due to the debugging capabilities. There are many configurations that may work for you, but consider branching out to some of these other tools as well.\nGraphics processing units (GPUs): GPUs are the workhorses of many modern machine learning algorithms, especially any that involve neural network-based architectures. There will be a small number of assignments that will require additional computation from that of GPUs. For these, we will use Google Colab, which is a free notebook environment that enables access to cloud resources including GPUs. For longer sessions before timeouts, greater RAM, and better GPUs you can optionally upgrade to Colab Pro."
  },
  {
    "objectID": "syllabus.html#basic-logistics",
    "href": "syllabus.html#basic-logistics",
    "title": "Course Logistics",
    "section": "",
    "text": "Class Time and Location\n\n\n\n\nWhen: Tuesday and Thursdays 10:15am - 11:30am\nWhere: French Science 2231\n\n\n\n\n\n\nEd Discussions: announcements, Q&A on course content (assignments, quizzes, grades), all course communications\nGradescope: Assignment & project submission & feedback\nCourse Schedule: Schedule & assignments\n\n\n\n\nEach book is available free online:\n\nAn Introduction to Statistical Learning with Python by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, 2013.\nPattern Recognition and Machine Learning by Christopher Bishop, 2006.\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, 2016.\nReinforcement Learning: An Introduction, by Richard Sutton and Andrew Barto, 2018.\n\n\n\n\nAssignments, projects, & quizzes: Assignments and projects details are posted on the course syllabus. For expectations and instructions on the assignments, see the Assignment Instructions. Quizzes are found on Sakai under “Tests & Quizzes”.\n\n\n\n55% Assignments (5, each worth ~11%)\n20% Quizzes (~23, each worth &lt;1%)\n25% Final Project\n\n\n\n\n\nThis course moves quickly, so having a firm grasp on prerequisites is important. The prerequisites are as follows:\n\nProgramming: Fundamentals of Python programming.\nMathematics: Calculus and linear algebra.\nStatistics: Introductory probability and statistics.\n\n\n\n\nProgramming language: We will use Python 3.x. The Anaconda distribution is recommended and comes with the most common packages. Python continues to be an one of the top programming languages and the rich packages in the language make it an excellent choice for machine learning. In particular the Python ecosystem of packages makes it a natural choice for ML including core numerical programming and plotting libraries like numpy, scipy, matplotlib, and pandas as well as excellent packages for machine learning algorithm development and statistical modeling including TensorFlow, Pytorch, Keras, Scikit-Learn.\nDevelopment environments: Jupyter lab or Jupyter notebook will be appropriate for most class assignments. We highly encourage you to use Visual Studio Code or Spyder are for larger projects, in particular due to the debugging capabilities. There are many configurations that may work for you, but consider branching out to some of these other tools as well.\nGraphics processing units (GPUs): GPUs are the workhorses of many modern machine learning algorithms, especially any that involve neural network-based architectures. There will be a small number of assignments that will require additional computation from that of GPUs. For these, we will use Google Colab, which is a free notebook environment that enables access to cloud resources including GPUs. For longer sessions before timeouts, greater RAM, and better GPUs you can optionally upgrade to Colab Pro."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Course Logistics",
    "section": "Course Policies",
    "text": "Course Policies\n\nAcademic dishonesty\nAdherence to the Duke Community Standard is expected. To uphold the Duke Community Standard:\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised\nAnyone found in violation of the Standard will be reported to the Office of Student Conduct.\n\n\nClass Attendance\nAttending class is a vital component of the course as it is one of the multiple ways in which you will interact with and learn course material. In person class attendance is therefore expected for this course. For any special circumstances, please reach out to the course instructor.\n\n\nSick absences\nTo keep the university community as safe and healthy as possible, please do not come to class if you have cold symptoms. Please inform me of your absence and plan to complete any missed work. Students who encounter short- and long-term medical issues or instances of personal distress or emergency can seek academic support if needed. Recordings of the class will be available for excused absences.\n\n\nAccommodations\nIf you need special accommodations due to physical or learning disabilities, medical needs, religious practices, or other reasons, please inform us as soon as possible so we can work to accommodate those needs.\n\n\nLate Submissions\nAssignments and projects are due in class by the start of class on the date posted. Late deliverables will ONLY be accepted at the discretion of the instructor. Any late assignments will result in a reduction of 5 points off the grade per day late. Course projects will not be accepted after the deadline. Quizzes will not be accepted after the deadline. While quizzes cannot be made up since the answers are discussed, the lowest two quizzes will be dropped at the end of the semester for each student to accommodate necessary absences and days when we’re off our game. Please reach out to the TA’s or instructor as early as possible to request any special accommodations.\n\n\nCollaboration\nThere will be three modes of collaboration ranging from fully-collaborative group projects, to fully-independent work. The three modes are as follows, and will be indicated throughout the course:\n\nMode 1: Team-based Assignment. Collaboration is expected with every member of the team contributing to a single deliverable. Applies to the Project\nMode 2: Individual Assignment – Collaboration Permitted. Students hand in individual work, but they may work with others if they provide citations of the help they received, such as a list of people who assisted/collaborated with them to produce the final product. Duplication or copying is not permissible, even in part, and constitutes a breach of the honor code. Applies to Assignments\nMode 3: Individual Assignment – No Collaboration Permitted. Students hand in individual work that is completed entirely independent of any discussion or help from other students. Clarifying questions to teaching assistants and instructors are both permissible and encouraged. Applies to Quizzes"
  },
  {
    "objectID": "syllabus.html#the-use-of-artificial-intelligence",
    "href": "syllabus.html#the-use-of-artificial-intelligence",
    "title": "Course Logistics",
    "section": "The use of Artificial Intelligence",
    "text": "The use of Artificial Intelligence\nThis class is all about artificial intelligence, however, the primary intelligence that we are working to expand is your own.\n\nChallenges of commercial AI tools\nQuoted excerpts below from the University of Delaware\nCommercial AI tools have significant and sometimes difficult-to-spot limitations. “For example, the current generation of Chat-GPT cannot perform synthesis of sources or even document the sources that were used to create its responses. So it may be useful for idea generation or generic recommendations but it has no ability to creatively and independently generate new insights and make novel connections. These tools may also produce information that is incorrect or biased so students must ensure that material they submit or share has been verified with appropriate sources and is reasonably free of biases.”\n\n\nResponsible use of AI tools\nThere are a number of ways that AI tools can be helpful:\n\nAs a way of detecting spelling and grammar issues (such as with grammarly)\nAs a way of identifying possible ways to structure writing (although NOT to do the writing for you)\nAs a way of brainstorming ideas (keeping in mind that what is generated by the AI may be incorrect or misleading)\n\nIn this class, the use of AI tools is acceptable WITH CITATION. An appropriate citation would indicate the following information:\n\nWhat AI tool you used (e.g. ChatGPT)\nHow you used it, specifying prompts used (e.g. I checked my writeup for clarity with the prompt, “Review this text {} and improve its clarity”)\n\nAt the end of the day, you are responsible for the correctness of everything that you submit, regardless of whether there was an AI tool used in its generation.\n\n\nAccessibility\nIn addition to accessibility issues experienced during the typical academic year, I recognize that remote learning may present additional challenges. Students may be experiencing unreliable wi-fi, lack of access to quiet study spaces, varied time-zones, or additional responsibilities while studying at home. If you are experiencing these or other difficulties, please contact me to discuss possible accommodations.\n\n\nRules for video recording course content\nStudent recording recordings of lectures must be permitted by the instructor and shall be for private study only. Such recordings shall not be distributed to anyone else without authorization by the instructor whose lecture has been recorded. However, the instructor may arrange through the Office of Information Technology to make recorded lectures available to students enrolled in the class on such terms and conditions as he or she prescribes. Redistribution of recorded lectures is prohibited. Unauthorized distribution is a cause for disciplinary action by the Judicial Board. The full policy on recoding of lectures falls under the Duke University Policy on Intellectual Property Rights, available here."
  },
  {
    "objectID": "syllabus.html#mental-health-and-wellness-resources",
    "href": "syllabus.html#mental-health-and-wellness-resources",
    "title": "Course Logistics",
    "section": "Mental Health and Wellness Resources",
    "text": "Mental Health and Wellness Resources\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to you, including ones listed below. Duke encourages all students to access these resources, particularly as we navigate the transition and emotions associated with this time. Duke Student Government has worked with DukeReach and student advocates to create the Fall 2020 “Two-Click Support” Form, and Duke Reach has expanded its drop in hours as well.\n\nDukeReach Provides comprehensive outreach services to identify and support students in managing all aspects of wellbeing. If you have concerns about a student’s behavior or health visit the website for resources and assistance.\nCounseling and Psychological Services (CAPS) CAPS services include individual, group, and couples counseling services, health coaching, psychiatric services, and workshops and discussions. (919) 660-1000\nBlue Devils Care A convenient and cost-effective way for Duke students to receive 24/7 mental health support through TalkNow.\n\nManaging daily stress and self-care are also important to well-being. Duke offers several resources for students to both seek assistance on coursework and improve overall wellness, some of which are listed below. Please visit this site to learn more about:\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu\nDuWell: (919) 681-8421, duwell@studentaffairs.duke.edu\nWellTrack"
  }
]